{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderState_ensemble(object):\n",
    "    \"\"\"\n",
    "    State of Decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, states):\n",
    "        \"\"\"\n",
    "        hidden: Tensor(num_layers, batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        self.states=states\n",
    "        self.mask=states[0].mask\n",
    "        \n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return self.__dict__.get(name)\n",
    "\n",
    "    def set_mask(self,):\n",
    "        for state in self.states:\n",
    "            state.mask=self.mask\n",
    "    \n",
    "    def get_batch_size(self):\n",
    "        \"\"\"\n",
    "        get_batch_size\n",
    "        \"\"\"\n",
    "        return self.states[0].get_batch_size()\n",
    "\n",
    "    def size(self):\n",
    "        \"\"\"\n",
    "        size\n",
    "        \"\"\"\n",
    "        return self.states[0].size()\n",
    "\n",
    "    def slice_select(self, stop):\n",
    "        \"\"\"\n",
    "        slice_select\n",
    "        \"\"\"\n",
    "        states=[]\n",
    "        for state in self.states:\n",
    "            states.append(state.slice_select(stop))\n",
    "        return DecoderState_ensemble(states)\n",
    "\n",
    "    def index_select(self, indices):\n",
    "        \"\"\"\n",
    "        index_select\n",
    "        \"\"\"\n",
    "        states=[]\n",
    "        for state in self.states:\n",
    "            states.append(state.index_select(indices))\n",
    "        return DecoderState_ensemble(states)\n",
    "\n",
    "    def mask_select(self, mask):\n",
    "        \"\"\"\n",
    "        mask_select\n",
    "        \"\"\"\n",
    "        states=[]\n",
    "        for state in self.states:\n",
    "            states.append(state.mask_select(mask))\n",
    "        return DecoderState_ensemble(states)\n",
    "\n",
    "    def inflate(self, times):\n",
    "        \"\"\"\n",
    "        inflate\n",
    "        \"\"\"\n",
    "        states=[]\n",
    "        for state in self.states:\n",
    "            states.append(state.inflate(times))\n",
    "            \n",
    "        return DecoderState_ensemble(states)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.utils.misc import Pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ensemble(nn.Module):\n",
    "    def __init__(self, models):\n",
    "        super(ensemble, self).__init__()\n",
    "        self.num=len(models)\n",
    "        self.models=nn.ModuleList(models)\n",
    "    def encode(self,inputs, enc_hidden):\n",
    "        enc_outputs=Pack()\n",
    "        states=[]\n",
    "        for i in range(self.num):\n",
    "            _, state = self.models[i].encode(inputs, enc_hidden)\n",
    "            states.append(state)\n",
    "#         print(len(states))\n",
    "        return enc_outputs, DecoderState_ensemble(states)\n",
    "    \n",
    "    def decode(self,input_var, dec_state):\n",
    "#         print('-----')\n",
    "        dec_state.set_mask()\n",
    "        p=Pack()\n",
    "        output_temp=[]\n",
    "        attn_temp=[]\n",
    "        dec_state_temp=[]\n",
    "#         print(len(dec_state.states))\n",
    "        for i in range(self.num):\n",
    "#             state_=dec_state.states[i]\n",
    "#             model_=self.models[i]\n",
    "#             print(type(state_))\n",
    "#             print(type(model_))\n",
    "#             output, dec_state_0, attn = model_.decode(input_var, state_)\n",
    "            output, dec_state_0, attn = self.models[i].decode(input_var, dec_state.states[i])\n",
    "            output_temp.append(output)\n",
    "            attn_temp.append(attn.attn)\n",
    "            dec_state_temp.append(dec_state_0)\n",
    "        \n",
    "        output_temp=torch.stack(output_temp).mean(0)\n",
    "        attn_temp=torch.stack(attn_temp).mean(0)\n",
    "        p.add(attn=attn_temp)\n",
    "        dec_state_temp=DecoderState_ensemble(dec_state_temp)\n",
    "        return output_temp, dec_state_temp, p\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file='./models/layer_2_64_embed_elmo/params.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_file=open(file,'r',encoding='utf-8')\n",
    "pp=json.loads(p_file.read())\n",
    "config=Pack(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/layer_2_64_embed_elmo/best.model'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.save_dir+'/best.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.inputters.corpus import KnowledgeCorpus, Entity_Corpus, Entity_Corpus_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.gpu=0\n",
    "device = config.gpu\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载实体成功，size= 321422\n",
      "Loading prepared vocab from ./data/data550_pos/model_50000.vocab.pt ...\n",
      "Finish loading vocab , size: 50005\n",
      "Finish loading pos vocab , size: 31\n",
      "Loading prepared data from ./data/data550_pos/model_50000.data.pt ...\n",
      "Number of examples: TRAIN-37226 VALID-2000\n"
     ]
    }
   ],
   "source": [
    "corpus = Entity_Corpus_pos(data_dir=config.data_dir, data_prefix=config.data_prefix, entity_file=config.entity_file,\n",
    "                             min_freq=config.min_freq, max_vocab_size=config.max_vocab_size)\n",
    "corpus.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_iter = corpus.create_batches(\n",
    "#         config.batch_size, \"valid\", shuffle=False, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_iter = corpus.create_batches(\n",
    "    5, \"valid\", shuffle=False, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'source.models.Entity_seq2seq_pos_elmo'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-bf9c36855cc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEntity_seq2seq_pos\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEntity_Seq2Seq_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEntity_seq2seq_pos_gru\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEntity_Seq2Seq_pos_gru\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEntity_seq2seq_pos_elmo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEntity_Seq2Seq_elmo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTopKGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'source.models.Entity_seq2seq_pos_elmo'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from source.models.Entity_seq2seq import Entity_Seq2Seq\n",
    "from source.models.Entity_seq2seq_pos import Entity_Seq2Seq_pos\n",
    "from source.models.Entity_seq2seq_pos_gru import Entity_Seq2Seq_pos_gru\n",
    "from source.models.Entity_seq2seq_pos_elmo import Entity_Seq2Seq_elmo\n",
    "from source.utils.generator import TopKGenerator\n",
    "from source.utils.engine import evaluate, evaluate_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file1='./models/layer_2_64_embed_10/params.json'\n",
    "file2='./models/layer_2_64_embed_11/params.json'\n",
    "file9='./models/layer_2_64_embed_13/params.json'\n",
    "file10='./models/layer_2_64_embed_14/params.json'\n",
    "file11='./models/layer_2_64_embed_15/params.json'\n",
    "file12='./models/layer_2_64_embed_16/params.json'\n",
    "files=[file1,file2,file9,file10,file11,file12]\n",
    "\n",
    "def get_model(files):\n",
    "    models=[]\n",
    "    for f in tqdm(files):\n",
    "        p_file=open(f,'r',encoding='utf-8')\n",
    "        pp=json.loads(p_file.read())\n",
    "        config=Pack(pp)\n",
    "        model = model = Entity_Seq2Seq_pos(src_vocab_size=corpus.SRC.vocab_size,\n",
    "                               pos_vocab_size=corpus.POS.vocab_size,\n",
    "                               embed_size=config.embed_size, hidden_size=config.hidden_size,\n",
    "                               padding_idx=corpus.padding_idx,\n",
    "                               num_layers=config.num_layers, bidirectional=config.bidirectional,\n",
    "                               attn_mode=config.attn, with_bridge=config.with_bridge,\n",
    "                               dropout=config.dropout,\n",
    "                               use_gpu=config.use_gpu,\n",
    "                               pretrain_epoch=config.pretrain_epoch)\n",
    "        config.ckpt=config.save_dir+'/best.model'\n",
    "        model.load(config.ckpt)\n",
    "        models.append(model)\n",
    "    return ensemble(models)\n",
    "\n",
    "# models=get_model(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:00<00:02,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model state from './models/layer_2_64_embed_18/best.model'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 2/4 [00:01<00:01,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model state from './models/layer_2_64_embed_19/best.model'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 3/4 [00:02<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model state from './models/layer_2_64_embed_20/best.model'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 4/4 [00:03<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model state from './models/layer_2_64_embed_21/best.model'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "models=get_model(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.max_dec_len=5\n",
    "config.beam_size=5\n",
    "generator = TopKGenerator(model=models,\n",
    "                              src_field=corpus.SRC,\n",
    "                              max_length=config.max_dec_len, ignore_unk=config.ignore_unk,\n",
    "\t\t\t      length_average=config.length_average, use_gpu=config.use_gpu, beam_size=config.beam_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generation results to './test.result'\n",
      "Avg_Len-2.293   F1-0.627   precision-0.643   recal-0.686   emo_precision-0.647\n",
      "Target:   AVG_LEN-2.093   \n"
     ]
    }
   ],
   "source": [
    "evaluate_generation(generator, valid_iter, save_file=config.gen_file, verbos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: TRAIN-37226 VALID-2000 TEST-40000\n"
     ]
    }
   ],
   "source": [
    "corpus.reload(data_type='test')\n",
    "test_iter = corpus.create_batches(\n",
    "            20, \"test\", shuffle=False, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg_Len-2.299   F1-0.000   precision-0.000   recal-0.000   emo_precision-0.000\n",
      "Target:   AVG_LEN-1.000   \n"
     ]
    }
   ],
   "source": [
    "evaluate_generation(generator, test_iter, save_file=config.gen_file, verbos=True, for_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ],
     "output_type": "error"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dutir923/irguest/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "Saved generation results to './test.result'\n",
    "Avg_Len-2.179   F1-0.631   precision-0.661   recal-0.674   emo_precision-0.635\n",
    "Target:   AVG_LEN-2.093   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Saved generation results to './test.result'\n",
    "Avg_Len-2.182   F1-0.633   precision-0.661   recal-0.676   emo_precision-0.637\n",
    "Target:   AVG_LEN-2.093   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1.0,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
