{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, warnings, random, datetime, math\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, KFold,GroupShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "########################### Helpers\n",
    "#################################################################################\n",
    "## Seeder\n",
    "# :seed to make all processes deterministic     # type: int\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "## Global frequency encoding    \n",
    "def frequency_encoding(df, columns, self_encoding=False):\n",
    "    for col in columns:\n",
    "        fq_encode = df[col].value_counts(dropna=False).to_dict()\n",
    "        if self_encoding:\n",
    "            df[col] = df[col].map(fq_encode)\n",
    "        else:\n",
    "            df[col+'_fq_enc'] = df[col].map(fq_encode)\n",
    "    return df\n",
    "\n",
    "\n",
    "def values_normalization(dt_df, periods, columns, enc_type='both'):\n",
    "    for period in periods:\n",
    "        for col in columns:\n",
    "            new_col = col +'_'+ period\n",
    "            dt_df[col] = dt_df[col].astype(float)  \n",
    "\n",
    "            temp_min = dt_df.groupby([period])[col].agg(['min']).reset_index()\n",
    "            temp_min.index = temp_min[period].values\n",
    "            temp_min = temp_min['min'].to_dict()\n",
    "\n",
    "            temp_max = dt_df.groupby([period])[col].agg(['max']).reset_index()\n",
    "            temp_max.index = temp_max[period].values\n",
    "            temp_max = temp_max['max'].to_dict()\n",
    "\n",
    "            temp_mean = dt_df.groupby([period])[col].agg(['mean']).reset_index()\n",
    "            temp_mean.index = temp_mean[period].values\n",
    "            temp_mean = temp_mean['mean'].to_dict()\n",
    "\n",
    "            temp_std = dt_df.groupby([period])[col].agg(['std']).reset_index()\n",
    "            temp_std.index = temp_std[period].values\n",
    "            temp_std = temp_std['std'].to_dict()\n",
    "\n",
    "            dt_df['temp_min'] = dt_df[period].map(temp_min)\n",
    "            dt_df['temp_max'] = dt_df[period].map(temp_max)\n",
    "            dt_df['temp_mean'] = dt_df[period].map(temp_mean)\n",
    "            dt_df['temp_std'] = dt_df[period].map(temp_std)\n",
    "            \n",
    "            if enc_type=='both':\n",
    "                dt_df[new_col+'_min_max'] = (dt_df[col]-dt_df['temp_min'])/(dt_df['temp_max']-dt_df['temp_min'])\n",
    "                dt_df[new_col+'_std_score'] = (dt_df[col]-dt_df['temp_mean'])/(dt_df['temp_std'])\n",
    "            elif enc_type=='norm':\n",
    "                 dt_df[new_col+'_std_score'] = (dt_df[col]-dt_df['temp_mean'])/(dt_df['temp_std'])\n",
    "            elif enc_type=='min_max':\n",
    "                dt_df[new_col+'_min_max'] = (dt_df[col]-dt_df['temp_min'])/(dt_df['temp_max']-dt_df['temp_min'])\n",
    "\n",
    "            del dt_df['temp_min'],dt_df['temp_max'],dt_df['temp_mean'],dt_df['temp_std']\n",
    "    return dt_df\n",
    "\n",
    "def get_new_columns(temp_list):\n",
    "    temp_list = [col for col in list(full_df) if col not in temp_list]\n",
    "    temp_list.sort()\n",
    "\n",
    "    temp_list2 = [col if col not in remove_features else '-' for col in temp_list ]\n",
    "    temp_list2.sort()\n",
    "\n",
    "    temp_list = {'New columns (including dummy)': temp_list,\n",
    "                 'New Features': temp_list2}\n",
    "    temp_list = pd.DataFrame.from_dict(temp_list)\n",
    "    return temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Vars\n",
    "#################################################################################\n",
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "LOCAL_TEST = True\n",
    "MAKE_TESTS = True\n",
    "TARGET = 'isFraud'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Model params\n",
    "lgb_params = {\n",
    "                    'objective':'binary',\n",
    "                    'boosting_type':'gbdt',\n",
    "                    'metric':'auc',\n",
    "                    'n_jobs':-1,\n",
    "                    'learning_rate':0.01,\n",
    "                    'num_leaves': 2**8,\n",
    "                    'max_depth':-1,\n",
    "                    'tree_learner':'serial',\n",
    "                    'colsample_bytree': 0.7,\n",
    "                    'subsample_freq':1,\n",
    "                    'subsample':0.7,\n",
    "                    'n_estimators':80000,\n",
    "                    'max_bin':255,\n",
    "                    'verbose':-1,\n",
    "                    'seed': SEED,\n",
    "                    'early_stopping_rounds':100, \n",
    "                } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "########################### Model\n",
    "import lightgbm as lgb\n",
    "\n",
    "def make_test(old_score=0, output=False):\n",
    "\n",
    "    features_columns = [col for col in list(full_df) if col not in remove_features]\n",
    "    train_mask = full_df['TransactionID'].isin(local_train_id['TransactionID'])\n",
    "    test_mask = full_df['TransactionID'].isin(local_test_id['TransactionID'])\n",
    "    \n",
    "    X,y = full_df[train_mask][features_columns], full_df[train_mask][TARGET]    \n",
    "    P,P_y = full_df[test_mask][features_columns], full_df[test_mask][TARGET]  \n",
    "\n",
    "    for col in list(X):\n",
    "        if X[col].dtype=='O':\n",
    "            X[col] = X[col].fillna('unseen_before_label')\n",
    "            P[col] = P[col].fillna('unseen_before_label')\n",
    "\n",
    "            X[col] = X[col].astype(str)\n",
    "            P[col] = P[col].astype(str)\n",
    "\n",
    "            le = LabelEncoder()\n",
    "            le.fit(list(X[col])+list(P[col]))\n",
    "            X[col] = le.transform(X[col])\n",
    "            P[col]  = le.transform(P[col])\n",
    "\n",
    "            X[col] = X[col].astype('category')\n",
    "            P[col] = P[col].astype('category')\n",
    "        \n",
    "    tt_df = full_df[test_mask][['TransactionID','DT_W',TARGET]]        \n",
    "    tt_df['prediction'] = 0\n",
    "    \n",
    "    tr_data = lgb.Dataset(X, label=y)\n",
    "    vl_data = lgb.Dataset(P, label=P_y) \n",
    "    estimator = lgb.train(\n",
    "            lgb_params,\n",
    "            tr_data,\n",
    "            valid_sets = [tr_data, vl_data],\n",
    "            verbose_eval = 200,\n",
    "        )   \n",
    "        \n",
    "    tt_df['prediction'] = estimator.predict(P)\n",
    "    feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(),X.columns)), columns=['Value','Feature'])\n",
    "    \n",
    "    if output:\n",
    "        tt_df[['TransactionID','prediction']].to_csv('oof.csv',index=False)\n",
    "        print('---Wrote OOF to file---')\n",
    "    \n",
    "    m_results = []\n",
    "    print('#'*20)\n",
    "    g_auc = metrics.roc_auc_score(tt_df[TARGET], tt_df['prediction'])\n",
    "    score_diff = g_auc - old_score\n",
    "    print('Global AUC', g_auc)\n",
    "    m_results.append(g_auc)\n",
    "    \n",
    "    for i in range(full_df[test_mask]['DT_W'].min(), full_df[test_mask]['DT_W'].max()+1):\n",
    "        mask = tt_df['DT_W']==i\n",
    "        w_auc = metrics.roc_auc_score(tt_df[mask][TARGET], tt_df[mask]['prediction'])\n",
    "        print('Week', i, w_auc, len(tt_df[mask]))\n",
    "        m_results.append(w_auc)\n",
    "        \n",
    "    print('#'*20)\n",
    "    print('Features Preformance:', g_auc)\n",
    "    print('Diff with previous__:', score_diff)\n",
    "    \n",
    "    return tt_df, feature_imp, m_results, estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Data\n",
      "Shape control (for local test): (417559, 1) (89326, 1)\n"
     ]
    }
   ],
   "source": [
    "########################### DATA LOAD\n",
    "#################################################################################\n",
    "print('Load Data')\n",
    "train_df = pd.read_pickle('../input/ieee-data-minification-private/train_transaction.pkl')\n",
    "test_df = pd.read_pickle('../input/ieee-data-minification-private/test_transaction.pkl')\n",
    "\n",
    "# Full Data set (careful with target encoding)\n",
    "full_df = pd.concat([train_df, test_df]).reset_index(drop=True)\n",
    "\n",
    "# Local test IDs with one month gap\n",
    "local_test_id  = train_df[train_df['DT_M']==train_df['DT_M'].max()].reset_index(drop=True)\n",
    "local_train_id = train_df[train_df['DT_M']<(train_df['DT_M'].max()-1)].reset_index(drop=True)\n",
    "local_train_id = local_train_id[['TransactionID']]\n",
    "local_test_id  = local_test_id[['TransactionID']]\n",
    "del train_df, test_df\n",
    "\n",
    "# Identity Data set\n",
    "train_identity = pd.read_pickle('../input/ieee-data-minification-private/train_identity.pkl')\n",
    "test_identity = pd.read_pickle('../input/ieee-data-minification-private/test_identity.pkl')\n",
    "identity_df = pd.concat([train_identity, test_identity]).reset_index(drop=True)\n",
    "del train_identity, test_identity\n",
    "\n",
    "print('Shape control (for local test):', local_train_id.shape, local_test_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### All features columns\n",
    "#################################################################################\n",
    "# Add list of feature that we will remove for sure\n",
    "remove_features = [\n",
    "    'TransactionID','TransactionDT', \n",
    "    TARGET,\n",
    "    'DT','DT_M','DT_W','DT_D','DTT',\n",
    "    'DT_hour','DT_day_week','DT_day_month',\n",
    "    'DT_M_total','DT_W_total','DT_D_total',\n",
    "    'is_december','is_holiday','temp','weight',\n",
    "    ]\n",
    "\n",
    "# Make sure that TransactionAmt is float64\n",
    "# To not lose values during aggregations\n",
    "full_df['TransactionAmt'] = full_df['TransactionAmt'].astype(float)\n",
    "\n",
    "# Base lists for features to do frequency encoding\n",
    "# and saved initial state\n",
    "fq_encode = []\n",
    "base_columns = list(full_df)\n",
    "\n",
    "# We don't need V columns in the initial phase \n",
    "# removing them to make predictions faster\n",
    "remove_features += ['V'+str(i) for i in range(1,340)]\n",
    "\n",
    "# Removing transformed D columns\n",
    "remove_features += ['uid_td_D'+str(i) for i in range(1,16) if i!=9]\n",
    "\n",
    "# Make sure we have m_results variable\n",
    "m_results = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.956002\tvalid_1's auc: 0.896727\n",
      "[400]\ttraining's auc: 0.982092\tvalid_1's auc: 0.909222\n",
      "[600]\ttraining's auc: 0.991172\tvalid_1's auc: 0.912425\n",
      "[800]\ttraining's auc: 0.995296\tvalid_1's auc: 0.912963\n",
      "[1000]\ttraining's auc: 0.997357\tvalid_1's auc: 0.913663\n",
      "Early stopping, best iteration is:\n",
      "[1029]\ttraining's auc: 0.997538\tvalid_1's auc: 0.913858\n",
      "####################\n",
      "Global AUC 0.9138583105064509\n",
      "Week 70 0.923483266765192 18970\n",
      "Week 71 0.9055766548653578 20726\n",
      "Week 72 0.9044171429729404 20332\n",
      "Week 73 0.9252377072819036 19010\n",
      "Week 74 0.9066227953475415 10288\n",
      "####################\n",
      "Features Preformance: 0.9138583105064509\n",
      "Diff with previous__: 0.9138583105064509\n"
     ]
    }
   ],
   "source": [
    "########################### This is start baseline\n",
    "if MAKE_TESTS:\n",
    "    tt_df, feature_imp, m_results, model = make_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fix card4 and card6 values\n",
      "Empty DataFrame\n",
      "Columns: [New columns (including dummy), New Features]\n",
      "Index: []\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.956134\tvalid_1's auc: 0.897406\n",
      "[400]\ttraining's auc: 0.982227\tvalid_1's auc: 0.909268\n",
      "[600]\ttraining's auc: 0.991256\tvalid_1's auc: 0.912698\n",
      "[800]\ttraining's auc: 0.995302\tvalid_1's auc: 0.913689\n",
      "Early stopping, best iteration is:\n",
      "[815]\ttraining's auc: 0.995495\tvalid_1's auc: 0.913836\n",
      "####################\n",
      "Global AUC 0.9138355439672678\n",
      "Week 70 0.9221205073979968 18970\n",
      "Week 71 0.905179855012491 20726\n",
      "Week 72 0.9056657960011959 20332\n",
      "Week 73 0.9242749819754866 19010\n",
      "Week 74 0.9086816993390571 10288\n",
      "####################\n",
      "Features Preformance: 0.9138355439672678\n",
      "Diff with previous__: -2.276653918309446e-05\n"
     ]
    }
   ],
   "source": [
    "########################### Fix card columns and encode\n",
    "print('Fix card4 and card6 values')\n",
    "saved_state = list(full_df)\n",
    "####\n",
    "\n",
    "####\n",
    "# card4 and card5 have strong connection\n",
    "# with card1 - we can unify values\n",
    "# to guarantee that it will be same combinations\n",
    "# for all data.\n",
    "\n",
    "# I've tried to fill others NaNs\n",
    "# But seems that there are no more bad values.\n",
    "# All rest NaNs are meaningful.\n",
    "####\n",
    "\n",
    "full_df['card6'] = np.where(full_df['card6']==30, np.nan, full_df['card6'])\n",
    "full_df['card6'] = np.where(full_df['card6']==16, np.nan, full_df['card6'])\n",
    "\n",
    "i_cols = ['card4','card6']\n",
    "\n",
    "for col in i_cols:\n",
    "    temp_df = full_df.groupby(['card1',col])[col].agg(['count']).reset_index()\n",
    "    temp_df = temp_df.sort_values(by=['card1','count'], ascending=False)\n",
    "    del temp_df['count']\n",
    "    temp_df = temp_df.drop_duplicates(subset=['card1'], keep='first').reset_index(drop=True)\n",
    "    temp_df.index = temp_df['card1'].values\n",
    "    temp_df = temp_df[col].to_dict()\n",
    "    full_df[col] = full_df['card1'].map(temp_df)\n",
    "    \n",
    "# Add cards features for later encoding\n",
    "i_cols = ['card1','card2','card3','card4','card5','card6']\n",
    "fq_encode += i_cols\n",
    "\n",
    "####\n",
    "if MAKE_TESTS:\n",
    "    print(get_new_columns(saved_state))\n",
    "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create client identification ID\n"
     ]
    }
   ],
   "source": [
    "########################### Client Virtual ID\n",
    "print('Create client identification ID')\n",
    "saved_state = list(full_df)\n",
    "####\n",
    "\n",
    "####\n",
    "# Client subgroups:\n",
    "\n",
    "# bank_type -> looking on card3 and card5 distributions\n",
    "# I would say it is bank branch and country\n",
    "# full_addr -> Client registration address in bank\n",
    "# uid1 -> client identification by bank and card type\n",
    "# uid2 -> client identification with additional geo information\n",
    "####\n",
    "\n",
    "# Bank type\n",
    "full_df['bank_type'] = full_df['card3'].astype(str)+'_'+full_df['card5'].astype(str)\n",
    "\n",
    "# Full address\n",
    "full_df['full_addr'] = full_df['addr1'].astype(str)+'_'+full_df['addr2'].astype(str)\n",
    "\n",
    "# Virtual client uid\n",
    "i_cols = ['card1','card2','card3','card4','card5','card6']\n",
    "full_df['uid1'] = ''\n",
    "for col in i_cols:\n",
    "    full_df['uid1'] += full_df[col].astype(str)+'_'\n",
    "\n",
    "# Virtual client uid + full_addr\n",
    "full_df['uid2'] = full_df['uid1']+'_'+full_df['full_addr'].astype(str)\n",
    "\n",
    "\n",
    "# Add uids features for later encoding\n",
    "i_cols = ['full_addr','bank_type','uid1','uid2']\n",
    "fq_encode += i_cols\n",
    "\n",
    "# We can't use this features directly because\n",
    "# test data will have many unknow values\n",
    "remove_features += i_cols\n",
    "\n",
    "# We've created just \"ghost\" features -> no need to run test\n",
    "if False: \n",
    "    print(get_new_columns(saved_state))\n",
    "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create client identification ID using deltas\n"
     ]
    }
   ],
   "source": [
    "########################### Client identification using deltas\n",
    "print('Create client identification ID using deltas')\n",
    "saved_state = list(full_df)\n",
    "####\n",
    "\n",
    "# Temporary list\n",
    "client_cols = []\n",
    "\n",
    "# Convert all delta columns to some date\n",
    "# D8 and D9 are not days deltas -\n",
    "# we can try convert D8 to int and \n",
    "# probably it will give us date\n",
    "# but I'm very very unsure about it.\n",
    "\n",
    "# We will do all D columns transformation\n",
    "# (but save original values) as we will\n",
    "# use it later for other features.\n",
    "\n",
    "for col in ['D'+str(i) for i in range(1,16) if i!=9]: \n",
    "    new_col = 'uid_td_'+str(col)\n",
    "    \n",
    "    new_col = 'uid_td_'+str(col)\n",
    "    full_df[new_col] = full_df['TransactionDT'] / (24*60*60)\n",
    "    full_df[new_col] = np.floor(full_df[new_col] - full_df[col])    \n",
    "    remove_features.append(new_col)\n",
    "    \n",
    "    # Date is useless itself -> add to dummy features\n",
    "    #remove_features.append(new_col)\n",
    "\n",
    "\n",
    "# The most possible deltas to identify account or client\n",
    "# initial activity are 'D1','D10','D15'\n",
    "# We can try to find certain client using uid and date\n",
    "# If client is the same uid+date combination will be\n",
    "# unique per client and all his transactions\n",
    "for col in ['D1','D10','D15']:\n",
    "    new_col = 'uid_td_'+str(col)\n",
    "\n",
    "    # card1 + full_addr + date\n",
    "    full_df[new_col+'_cUID_1'] = full_df['card1'].astype(str)+'_'+full_df['full_addr'].astype(str)+'_'+full_df[new_col].astype(str)\n",
    "    \n",
    "    # uid1 + full_addr + date\n",
    "    full_df[new_col+'_cUID_2'] = full_df['uid2'].astype(str)+'_'+full_df[new_col].astype(str)\n",
    "\n",
    "    # columns 'D1','D2' are clipped we can't trust maximum values\n",
    "    if col in ['D1','D2']:\n",
    "        full_df[new_col+'_cUID_1'] = np.where(full_df[col]>=640, 'very_old_client', full_df[new_col+'_cUID_1'])\n",
    "        full_df[new_col+'_cUID_2'] = np.where(full_df[col]>=640, 'very_old_client', full_df[new_col+'_cUID_2'])\n",
    "\n",
    "    full_df[new_col+'_cUID_1'] = np.where(full_df[col].isna(), np.nan, full_df[new_col+'_cUID_1'])\n",
    "    full_df[new_col+'_cUID_2'] = np.where(full_df[col].isna(), np.nan, full_df[new_col+'_cUID_2'])\n",
    "\n",
    "    # reset cUID_1 if both address are nan (very unstable prediction)\n",
    "    full_df[new_col+'_cUID_1'] = np.where(full_df['addr1'].isna()&full_df['addr2'].isna(), np.nan, full_df[new_col+'_cUID_1'])\n",
    "\n",
    "    # cUID is useless itself -> add to dummy features\n",
    "    remove_features += [new_col+'_cUID_1',new_col+'_cUID_2']\n",
    "    \n",
    "    # Add to temporary list (to join with encoding list later)\n",
    "    client_cols += [new_col+'_cUID_1',new_col+'_cUID_2']\n",
    "    \n",
    "## Best candidate for client complete identification\n",
    "## uid_td_D1_cUID_1\n",
    "        \n",
    "# Add cUIDs features for later encoding\n",
    "fq_encode += client_cols\n",
    "\n",
    "# We will save this list and even append \n",
    "# few more columns for later use\n",
    "client_cols += ['card1','card2','card3','card4','card5',\n",
    "                'uid1','uid2']\n",
    "\n",
    "####\n",
    "# We've created just \"ghost\" features -> no need to run test\n",
    "if False: \n",
    "    print(get_new_columns(saved_state))\n",
    "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers mark\n",
      "            New columns (including dummy)  \\\n",
      "0               card1_catboost_check_DT_M   \n",
      "1               card2_catboost_check_DT_M   \n",
      "2               card3_catboost_check_DT_M   \n",
      "3               card4_catboost_check_DT_M   \n",
      "4               card5_catboost_check_DT_M   \n",
      "5                uid1_catboost_check_DT_M   \n",
      "6                uid2_catboost_check_DT_M   \n",
      "7   uid_td_D10_cUID_1_catboost_check_DT_M   \n",
      "8   uid_td_D10_cUID_2_catboost_check_DT_M   \n",
      "9   uid_td_D15_cUID_1_catboost_check_DT_M   \n",
      "10  uid_td_D15_cUID_2_catboost_check_DT_M   \n",
      "11   uid_td_D1_cUID_1_catboost_check_DT_M   \n",
      "12   uid_td_D1_cUID_2_catboost_check_DT_M   \n",
      "\n",
      "                             New Features  \n",
      "0               card1_catboost_check_DT_M  \n",
      "1               card2_catboost_check_DT_M  \n",
      "2               card3_catboost_check_DT_M  \n",
      "3               card4_catboost_check_DT_M  \n",
      "4               card5_catboost_check_DT_M  \n",
      "5                uid1_catboost_check_DT_M  \n",
      "6                uid2_catboost_check_DT_M  \n",
      "7   uid_td_D10_cUID_1_catboost_check_DT_M  \n",
      "8   uid_td_D10_cUID_2_catboost_check_DT_M  \n",
      "9   uid_td_D15_cUID_1_catboost_check_DT_M  \n",
      "10  uid_td_D15_cUID_2_catboost_check_DT_M  \n",
      "11   uid_td_D1_cUID_1_catboost_check_DT_M  \n",
      "12   uid_td_D1_cUID_2_catboost_check_DT_M  \n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.955955\tvalid_1's auc: 0.894931\n",
      "[400]\ttraining's auc: 0.982584\tvalid_1's auc: 0.908257\n",
      "[600]\ttraining's auc: 0.991496\tvalid_1's auc: 0.912199\n",
      "[800]\ttraining's auc: 0.995621\tvalid_1's auc: 0.912993\n",
      "Early stopping, best iteration is:\n",
      "[837]\ttraining's auc: 0.996076\tvalid_1's auc: 0.913198\n",
      "####################\n",
      "Global AUC 0.9131984980580351\n",
      "Week 70 0.9218732596411674 18970\n",
      "Week 71 0.9050205373427131 20726\n",
      "Week 72 0.9036098309472691 20332\n",
      "Week 73 0.9249005046863734 19010\n",
      "Week 74 0.9063057526142286 10288\n",
      "####################\n",
      "Features Preformance: 0.9131984980580351\n",
      "Diff with previous__: -0.0006370459092327474\n"
     ]
    }
   ],
   "source": [
    "########################### Mark card columns \"outliers\"\n",
    "print('Outliers mark')\n",
    "saved_state = list(full_df)\n",
    "####\n",
    "\n",
    "####\n",
    "# We are checking card and uid activity -\n",
    "# weither activity is constant during the year\n",
    "# or we have just single card/account use cases.\n",
    "\n",
    "# These features are categorical ones and\n",
    "# Catboost benefits the most from them.\n",
    "\n",
    "# Strange things:\n",
    "# - \"Time window\" should be big enough \n",
    "# - Doesn't work for DT_W and DT_D\n",
    "# even when local test showing score boost.\n",
    "\n",
    "# Seems to me that catboost start to combine \n",
    "# them with themselfs and loosing \"magic\".\n",
    "####\n",
    "\n",
    "i_cols = client_cols.copy()\n",
    "periods = ['DT_M'] \n",
    "\n",
    "for period in periods:\n",
    "    for col in i_cols:\n",
    "        full_df[col+'_catboost_check_'+period] = full_df.groupby([col])[period].transform('nunique')\n",
    "        full_df[col+'_catboost_check_'+period] = np.where(full_df[col+'_catboost_check_'+period]==1,1,0)\n",
    "        \n",
    "####\n",
    "if MAKE_TESTS:\n",
    "    print(get_new_columns(saved_state))\n",
    "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V columns / Nan groups\n",
      "   New columns (including dummy)           New Features\n",
      "0      nan_group_catboost_101245                      -\n",
      "1          nan_group_catboost_15                      -\n",
      "2      nan_group_catboost_245823                      -\n",
      "3         nan_group_catboost_314                      -\n",
      "4      nan_group_catboost_455805                      -\n",
      "5        nan_group_catboost_7300                      -\n",
      "6      nan_group_catboost_818499                      -\n",
      "7      nan_group_catboost_820866                      -\n",
      "8      nan_group_catboost_821037                      -\n",
      "9      nan_group_catboost_840073                      -\n",
      "10      nan_group_catboost_88662                      -\n",
      "11      nan_group_catboost_89995                      -\n",
      "12     nan_group_catboost_938449                      -\n",
      "13     nan_group_catboost_939225                      -\n",
      "14     nan_group_catboost_939501                      -\n",
      "15         nan_group_mean_101245  nan_group_mean_101245\n",
      "16             nan_group_mean_15      nan_group_mean_15\n",
      "17         nan_group_mean_245823  nan_group_mean_245823\n",
      "18            nan_group_mean_314     nan_group_mean_314\n",
      "19         nan_group_mean_455805  nan_group_mean_455805\n",
      "20           nan_group_mean_7300    nan_group_mean_7300\n",
      "21         nan_group_mean_818499  nan_group_mean_818499\n",
      "22         nan_group_mean_820866  nan_group_mean_820866\n",
      "23         nan_group_mean_821037  nan_group_mean_821037\n",
      "24         nan_group_mean_840073  nan_group_mean_840073\n",
      "25          nan_group_mean_88662   nan_group_mean_88662\n",
      "26          nan_group_mean_89995   nan_group_mean_89995\n",
      "27         nan_group_mean_938449  nan_group_mean_938449\n",
      "28         nan_group_mean_939225  nan_group_mean_939225\n",
      "29         nan_group_mean_939501  nan_group_mean_939501\n",
      "30          nan_group_sum_101245   nan_group_sum_101245\n",
      "31              nan_group_sum_15       nan_group_sum_15\n",
      "32          nan_group_sum_245823   nan_group_sum_245823\n",
      "33             nan_group_sum_314      nan_group_sum_314\n",
      "34          nan_group_sum_455805   nan_group_sum_455805\n",
      "35            nan_group_sum_7300     nan_group_sum_7300\n",
      "36          nan_group_sum_818499   nan_group_sum_818499\n",
      "37          nan_group_sum_820866   nan_group_sum_820866\n",
      "38          nan_group_sum_821037   nan_group_sum_821037\n",
      "39          nan_group_sum_840073   nan_group_sum_840073\n",
      "40           nan_group_sum_88662    nan_group_sum_88662\n",
      "41           nan_group_sum_89995    nan_group_sum_89995\n",
      "42          nan_group_sum_938449   nan_group_sum_938449\n",
      "43          nan_group_sum_939225   nan_group_sum_939225\n",
      "44          nan_group_sum_939501   nan_group_sum_939501\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.958349\tvalid_1's auc: 0.895602\n",
      "[400]\ttraining's auc: 0.983446\tvalid_1's auc: 0.909369\n",
      "[600]\ttraining's auc: 0.992342\tvalid_1's auc: 0.914153\n",
      "[800]\ttraining's auc: 0.996172\tvalid_1's auc: 0.915021\n",
      "[1000]\ttraining's auc: 0.997947\tvalid_1's auc: 0.915091\n",
      "Early stopping, best iteration is:\n",
      "[1068]\ttraining's auc: 0.998303\tvalid_1's auc: 0.915266\n",
      "####################\n",
      "Global AUC 0.9152657571791852\n",
      "Week 70 0.9208804374883666 18970\n",
      "Week 71 0.9055479179119347 20726\n",
      "Week 72 0.9065624143421961 20332\n",
      "Week 73 0.9291363374188897 19010\n",
      "Week 74 0.910406821540418 10288\n",
      "####################\n",
      "Features Preformance: 0.9152657571791852\n",
      "Diff with previous__: 0.0020672591211501334\n"
     ]
    }
   ],
   "source": [
    "########################### V columns compact and assign groups\n",
    "print('V columns / Nan groups')\n",
    "saved_state = list(full_df)\n",
    "####\n",
    "\n",
    "####\n",
    "# Nangroups identification are categorical features\n",
    "# and Catboost benefits the most from them.\n",
    "\n",
    "# Mean/std just occasion transformation.\n",
    "####\n",
    "\n",
    "nans_groups = {}\n",
    "nans_df = full_df.isna()\n",
    "\n",
    "i_cols = ['V'+str(i) for i in range(1,340)]\n",
    "for col in i_cols:\n",
    "    cur_group = nans_df[col].sum()\n",
    "    try:\n",
    "        nans_groups[cur_group].append(col)\n",
    "    except:\n",
    "        nans_groups[cur_group]=[col]\n",
    "\n",
    "for col in nans_groups:\n",
    "    # Very doubtful features -> Seems it works in tandem with other feature\n",
    "    # But I'm not sure\n",
    "    full_df['nan_group_sum_'+str(col)] = full_df[nans_groups[col]].to_numpy().sum(axis=1)\n",
    "    full_df['nan_group_mean_'+str(col)] = full_df[nans_groups[col]].to_numpy().mean(axis=1)\n",
    "        \n",
    "    # lgbm doesn't benefit from such feature -> \n",
    "    # let's transform and add it to dummy features list\n",
    "    full_df['nan_group_catboost_'+str(col)]  = np.where(nans_df[nans_groups[col]].sum(axis=1)>0,1,0).astype(np.int8)\n",
    "    remove_features.append('nan_group_catboost_'+str(col))\n",
    "        \n",
    "####\n",
    "if MAKE_TESTS:\n",
    "    print(get_new_columns(saved_state))\n",
    "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean encoding, using M columns\n",
      "   New columns (including dummy)              New Features\n",
      "0                  card1_M2_mean             card1_M2_mean\n",
      "1                  card1_M3_mean             card1_M3_mean\n",
      "2                  card1_M5_mean             card1_M5_mean\n",
      "3                  card1_M6_mean             card1_M6_mean\n",
      "4                  card1_M7_mean             card1_M7_mean\n",
      "..                           ...                       ...\n",
      "56      uid_td_D1_cUID_2_M2_mean  uid_td_D1_cUID_2_M2_mean\n",
      "57      uid_td_D1_cUID_2_M3_mean  uid_td_D1_cUID_2_M3_mean\n",
      "58      uid_td_D1_cUID_2_M5_mean  uid_td_D1_cUID_2_M5_mean\n",
      "59      uid_td_D1_cUID_2_M6_mean  uid_td_D1_cUID_2_M6_mean\n",
      "60      uid_td_D1_cUID_2_M9_mean  uid_td_D1_cUID_2_M9_mean\n",
      "\n",
      "[61 rows x 2 columns]\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.970881\tvalid_1's auc: 0.907326\n",
      "[400]\ttraining's auc: 0.991844\tvalid_1's auc: 0.92208\n",
      "[600]\ttraining's auc: 0.996975\tvalid_1's auc: 0.925967\n",
      "[800]\ttraining's auc: 0.998608\tvalid_1's auc: 0.926735\n",
      "[1000]\ttraining's auc: 0.999311\tvalid_1's auc: 0.926773\n",
      "Early stopping, best iteration is:\n",
      "[933]\ttraining's auc: 0.999118\tvalid_1's auc: 0.927038\n",
      "####################\n",
      "Global AUC 0.9270384046186753\n",
      "Week 70 0.9278360794093263 18970\n",
      "Week 71 0.9211326957561266 20726\n",
      "Week 72 0.9160427489991259 20332\n",
      "Week 73 0.942109805335256 19010\n",
      "Week 74 0.9251854100381876 10288\n",
      "####################\n",
      "Features Preformance: 0.9270384046186753\n",
      "Diff with previous__: 0.011772647439490025\n"
     ]
    }
   ],
   "source": [
    "########################### Mean encoding using M columns\n",
    "print('Mean encoding, using M columns')\n",
    "saved_state = list(full_df)\n",
    "####\n",
    "\n",
    "main_cols = {\n",
    "             'uid_td_D1_cUID_1':   ['M'+str(i) for i in [2,3,5,7,8,9]],\n",
    "             'uid_td_D1_cUID_2':   ['M'+str(i) for i in [2,3,5,6,9]],\n",
    "             'uid_td_D10_cUID_1':  ['M'+str(i) for i in [5,7,8,9]],\n",
    "             'uid_td_D10_cUID_2':  ['M'+str(i) for i in [3,6,7,8]],\n",
    "             'uid_td_D15_cUID_1':  ['M'+str(i) for i in [2,3,5,6,8,]],\n",
    "             'uid_td_D15_cUID_2':  ['M'+str(i) for i in [2,3,5,6,7,8]],\n",
    "             'card1':  ['M'+str(i) for i in [2,3,5,6,7,8,9]],\n",
    "             'card2':  ['M'+str(i) for i in [1,2,3,7,9]],\n",
    "             'card4':  ['M'+str(i) for i in [3,7,8]],\n",
    "             'card5':  ['M'+str(i) for i in [5,6,8]],\n",
    "             'uid1':   ['M'+str(i) for i in [3,5,6,7,8,9]],\n",
    "             'uid2':   ['M'+str(i) for i in [2,3,5,6,7,8,9]],\n",
    "            }\n",
    "\n",
    "for main_col,i_cols in main_cols.items():\n",
    "    for agg_type in ['mean']:\n",
    "        temp_df = full_df[[main_col]+i_cols]\n",
    "        temp_df = temp_df.groupby([main_col])[i_cols].transform(agg_type)\n",
    "        temp_df.columns = [main_col+'_'+col+'_'+agg_type for col in list(temp_df)]\n",
    "        full_df = pd.concat([full_df,temp_df], axis=1)\n",
    "        \n",
    "####\n",
    "if MAKE_TESTS:\n",
    "    print(get_new_columns(saved_state))\n",
    "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D columns Mean/Std\n",
      "uid_td_D1_cUID_1\n",
      "   New columns (including dummy)               New Features\n",
      "0      uid_td_D1_cUID_1_D10_mean  uid_td_D1_cUID_1_D10_mean\n",
      "1       uid_td_D1_cUID_1_D10_std   uid_td_D1_cUID_1_D10_std\n",
      "2      uid_td_D1_cUID_1_D11_mean  uid_td_D1_cUID_1_D11_mean\n",
      "3       uid_td_D1_cUID_1_D11_std   uid_td_D1_cUID_1_D11_std\n",
      "4      uid_td_D1_cUID_1_D14_mean  uid_td_D1_cUID_1_D14_mean\n",
      "5       uid_td_D1_cUID_1_D14_std   uid_td_D1_cUID_1_D14_std\n",
      "6      uid_td_D1_cUID_1_D15_mean  uid_td_D1_cUID_1_D15_mean\n",
      "7       uid_td_D1_cUID_1_D15_std   uid_td_D1_cUID_1_D15_std\n",
      "8       uid_td_D1_cUID_1_D1_mean   uid_td_D1_cUID_1_D1_mean\n",
      "9        uid_td_D1_cUID_1_D1_std    uid_td_D1_cUID_1_D1_std\n",
      "10      uid_td_D1_cUID_1_D2_mean   uid_td_D1_cUID_1_D2_mean\n",
      "11       uid_td_D1_cUID_1_D2_std    uid_td_D1_cUID_1_D2_std\n",
      "12      uid_td_D1_cUID_1_D3_mean   uid_td_D1_cUID_1_D3_mean\n",
      "13       uid_td_D1_cUID_1_D3_std    uid_td_D1_cUID_1_D3_std\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.979417\tvalid_1's auc: 0.92185\n",
      "[400]\ttraining's auc: 0.994106\tvalid_1's auc: 0.931564\n",
      "[600]\ttraining's auc: 0.997821\tvalid_1's auc: 0.933873\n",
      "[800]\ttraining's auc: 0.998995\tvalid_1's auc: 0.934234\n",
      "Early stopping, best iteration is:\n",
      "[745]\ttraining's auc: 0.998768\tvalid_1's auc: 0.93435\n",
      "####################\n",
      "Global AUC 0.9343503264092958\n",
      "Week 70 0.9389689562967631 18970\n",
      "Week 71 0.9279219734240656 20726\n",
      "Week 72 0.9280478357617042 20332\n",
      "Week 73 0.9451338860850758 19010\n",
      "Week 74 0.9283078687308173 10288\n",
      "####################\n",
      "Features Preformance: 0.9343503264092958\n",
      "Diff with previous__: 0.007311921790620568\n"
     ]
    }
   ],
   "source": [
    "########################### D Columns Mean/Std\n",
    "print('D columns Mean/Std')\n",
    "saved_state = list(full_df)\n",
    "####\n",
    "\n",
    "i_cols = ['D'+str(i) for i in range(1,16)]\n",
    "main_cols = {\n",
    "             'uid_td_D1_cUID_1': ['D'+str(i) for i in [1,2,3,10,11,14,15]],\n",
    "            }\n",
    "\n",
    "for main_col,i_cols in main_cols.items():\n",
    "    print(main_col)\n",
    "    for agg_type in ['mean','std']:\n",
    "        temp_df = full_df.groupby([main_col])[i_cols].transform(agg_type)\n",
    "        temp_df.columns = [main_col+'_'+col+'_'+agg_type for col in list(temp_df)]\n",
    "        full_df = pd.concat([full_df,temp_df], axis=1)\n",
    "        \n",
    "####\n",
    "if MAKE_TESTS:\n",
    "    print(get_new_columns(saved_state))\n",
    "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransactionAmt normalization\n",
      "    New columns (including dummy)                    New Features\n",
      "0            TransactionAmt_cents            TransactionAmt_cents\n",
      "1              card1_Product_norm              card1_Product_norm\n",
      "2              card3_Product_norm              card3_Product_norm\n",
      "3  uid_td_D10_cUID_1_Product_norm  uid_td_D10_cUID_1_Product_norm\n",
      "4  uid_td_D10_cUID_2_Product_norm  uid_td_D10_cUID_2_Product_norm\n",
      "5  uid_td_D15_cUID_1_Product_norm  uid_td_D15_cUID_1_Product_norm\n",
      "6  uid_td_D15_cUID_2_Product_norm  uid_td_D15_cUID_2_Product_norm\n",
      "7   uid_td_D1_cUID_1_Product_norm   uid_td_D1_cUID_1_Product_norm\n",
      "8   uid_td_D1_cUID_2_Product_norm   uid_td_D1_cUID_2_Product_norm\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.98013\tvalid_1's auc: 0.920916\n",
      "[400]\ttraining's auc: 0.994535\tvalid_1's auc: 0.93026\n",
      "[600]\ttraining's auc: 0.998396\tvalid_1's auc: 0.932446\n",
      "[800]\ttraining's auc: 0.999453\tvalid_1's auc: 0.933066\n",
      "[1000]\ttraining's auc: 0.999808\tvalid_1's auc: 0.933484\n",
      "Early stopping, best iteration is:\n",
      "[1026]\ttraining's auc: 0.999833\tvalid_1's auc: 0.933545\n",
      "####################\n",
      "Global AUC 0.9335806222005762\n",
      "Week 70 0.9391542519512577 18970\n",
      "Week 71 0.9256167333363987 20726\n",
      "Week 72 0.9251089753913462 20332\n",
      "Week 73 0.9471051910598413 19010\n",
      "Week 74 0.9267321488576844 10288\n",
      "####################\n",
      "Features Preformance: 0.9335806222005762\n",
      "Diff with previous__: -0.0007697042087195793\n"
     ]
    }
   ],
   "source": [
    "########################### TransactionAmt\n",
    "print('TransactionAmt normalization')\n",
    "saved_state = list(full_df)\n",
    "####\n",
    "\n",
    "# Decimal part\n",
    "full_df['TransactionAmt_cents'] = np.round(100.*(full_df['TransactionAmt'] - np.floor(full_df['TransactionAmt'])),0)\n",
    "full_df['TransactionAmt_cents'] = full_df['TransactionAmt_cents'].astype(np.int8)\n",
    "\n",
    "# Clip top values\n",
    "full_df['TransactionAmt'] = full_df['TransactionAmt'].clip(0,5000)\n",
    "\n",
    "# Normalization by product\n",
    "main_cols = [\n",
    "             'uid_td_D1_cUID_1','uid_td_D1_cUID_2',\n",
    "             'uid_td_D10_cUID_1','uid_td_D10_cUID_2',\n",
    "             'uid_td_D15_cUID_1','uid_td_D15_cUID_2',\n",
    "             'card1','card3',\n",
    "            ]\n",
    "\n",
    "for col in main_cols:\n",
    "    for agg_type in ['mean','std']:\n",
    "        full_df[col+'_TransactionAmt_Product_' + agg_type] =\\\n",
    "                full_df.groupby([col,'ProductCD'])['TransactionAmt'].transform(agg_type)\n",
    "\n",
    "    f_std = col+'_TransactionAmt_Product_std'\n",
    "    f_mean = col+'_TransactionAmt_Product_mean'\n",
    "    full_df[col+'_Product_norm'] = (full_df['TransactionAmt']-full_df[f_mean])/full_df[f_std]\n",
    "    del full_df[f_mean], full_df[f_std]\n",
    "    \n",
    "\n",
    "####\n",
    "if MAKE_TESTS:\n",
    "    print(get_new_columns(saved_state))\n",
    "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransactionAmt encoding clients columns\n",
      "uid_td_D1_cUID_1\n",
      "uid_td_D1_cUID_2\n",
      "uid_td_D10_cUID_1\n",
      "uid_td_D10_cUID_2\n",
      "uid_td_D15_cUID_1\n",
      "uid_td_D15_cUID_2\n",
      "card1\n",
      "card2\n",
      "card3\n",
      "card4\n",
      "card5\n",
      "uid1\n",
      "uid2\n",
      "            New columns (including dummy)  \\\n",
      "0               card1_TransactionAmt_mean   \n",
      "1                card1_TransactionAmt_std   \n",
      "2               card2_TransactionAmt_mean   \n",
      "3                card2_TransactionAmt_std   \n",
      "4               card3_TransactionAmt_mean   \n",
      "5                card3_TransactionAmt_std   \n",
      "6               card4_TransactionAmt_mean   \n",
      "7                card4_TransactionAmt_std   \n",
      "8               card5_TransactionAmt_mean   \n",
      "9                card5_TransactionAmt_std   \n",
      "10               uid1_TransactionAmt_mean   \n",
      "11                uid1_TransactionAmt_std   \n",
      "12               uid2_TransactionAmt_mean   \n",
      "13                uid2_TransactionAmt_std   \n",
      "14  uid_td_D10_cUID_1_TransactionAmt_mean   \n",
      "15   uid_td_D10_cUID_1_TransactionAmt_std   \n",
      "16  uid_td_D10_cUID_2_TransactionAmt_mean   \n",
      "17   uid_td_D10_cUID_2_TransactionAmt_std   \n",
      "18  uid_td_D15_cUID_1_TransactionAmt_mean   \n",
      "19   uid_td_D15_cUID_1_TransactionAmt_std   \n",
      "20  uid_td_D15_cUID_2_TransactionAmt_mean   \n",
      "21   uid_td_D15_cUID_2_TransactionAmt_std   \n",
      "22   uid_td_D1_cUID_1_TransactionAmt_mean   \n",
      "23    uid_td_D1_cUID_1_TransactionAmt_std   \n",
      "24   uid_td_D1_cUID_2_TransactionAmt_mean   \n",
      "25    uid_td_D1_cUID_2_TransactionAmt_std   \n",
      "\n",
      "                             New Features  \n",
      "0               card1_TransactionAmt_mean  \n",
      "1                card1_TransactionAmt_std  \n",
      "2               card2_TransactionAmt_mean  \n",
      "3                card2_TransactionAmt_std  \n",
      "4               card3_TransactionAmt_mean  \n",
      "5                card3_TransactionAmt_std  \n",
      "6               card4_TransactionAmt_mean  \n",
      "7                card4_TransactionAmt_std  \n",
      "8               card5_TransactionAmt_mean  \n",
      "9                card5_TransactionAmt_std  \n",
      "10               uid1_TransactionAmt_mean  \n",
      "11                uid1_TransactionAmt_std  \n",
      "12               uid2_TransactionAmt_mean  \n",
      "13                uid2_TransactionAmt_std  \n",
      "14  uid_td_D10_cUID_1_TransactionAmt_mean  \n",
      "15   uid_td_D10_cUID_1_TransactionAmt_std  \n",
      "16  uid_td_D10_cUID_2_TransactionAmt_mean  \n",
      "17   uid_td_D10_cUID_2_TransactionAmt_std  \n",
      "18  uid_td_D15_cUID_1_TransactionAmt_mean  \n",
      "19   uid_td_D15_cUID_1_TransactionAmt_std  \n",
      "20  uid_td_D15_cUID_2_TransactionAmt_mean  \n",
      "21   uid_td_D15_cUID_2_TransactionAmt_std  \n",
      "22   uid_td_D1_cUID_1_TransactionAmt_mean  \n",
      "23    uid_td_D1_cUID_1_TransactionAmt_std  \n",
      "24   uid_td_D1_cUID_2_TransactionAmt_mean  \n",
      "25    uid_td_D1_cUID_2_TransactionAmt_std  \n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.982086\tvalid_1's auc: 0.923665\n",
      "[400]\ttraining's auc: 0.995684\tvalid_1's auc: 0.931944\n",
      "[600]\ttraining's auc: 0.998943\tvalid_1's auc: 0.934388\n",
      "[800]\ttraining's auc: 0.999684\tvalid_1's auc: 0.934916\n",
      "[1000]\ttraining's auc: 0.999901\tvalid_1's auc: 0.935111\n",
      "Early stopping, best iteration is:\n",
      "[980]\ttraining's auc: 0.999889\tvalid_1's auc: 0.935173\n",
      "####################\n",
      "Global AUC 0.9351652619801388\n",
      "Week 70 0.9386101386904802 18970\n",
      "Week 71 0.9298555489141264 20726\n",
      "Week 72 0.9302238239527928 20332\n",
      "Week 73 0.9454038211968275 19010\n",
      "Week 74 0.9269909796470558 10288\n",
      "####################\n",
      "Features Preformance: 0.9351652619801388\n",
      "Diff with previous__: 0.00158463977956258\n"
     ]
    }
   ],
   "source": [
    "########################### TransactionAmt clients columns encoding\n",
    "print('TransactionAmt encoding clients columns')\n",
    "saved_state = list(full_df)\n",
    "####\n",
    "\n",
    "i_cols = ['TransactionAmt']\n",
    "main_cols = client_cols.copy()\n",
    "\n",
    "for main_col in main_cols:\n",
    "    print(main_col)\n",
    "    for agg_type in ['mean','std']:\n",
    "        temp_df = full_df.groupby([main_col])[i_cols].transform(agg_type)\n",
    "        temp_df.columns = [main_col+'_'+col+'_'+agg_type for col in list(temp_df)]\n",
    "        full_df = pd.concat([full_df,temp_df], axis=1)\n",
    "\n",
    "####\n",
    "if MAKE_TESTS:\n",
    "    print(get_new_columns(saved_state))\n",
    "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical outliers\n",
      "        New columns (including dummy)                        New Features\n",
      "0   P_emaildomain_catboost_check_DT_M   P_emaildomain_catboost_check_DT_M\n",
      "1       ProductCD_catboost_check_DT_M       ProductCD_catboost_check_DT_M\n",
      "2   R_emaildomain_catboost_check_DT_M   R_emaildomain_catboost_check_DT_M\n",
      "3  TransactionAmt_catboost_check_DT_M  TransactionAmt_catboost_check_DT_M\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.981898\tvalid_1's auc: 0.922836\n",
      "[400]\ttraining's auc: 0.995741\tvalid_1's auc: 0.932705\n",
      "[600]\ttraining's auc: 0.998932\tvalid_1's auc: 0.934558\n",
      "[800]\ttraining's auc: 0.999675\tvalid_1's auc: 0.935346\n",
      "[1000]\ttraining's auc: 0.999905\tvalid_1's auc: 0.935822\n",
      "Early stopping, best iteration is:\n",
      "[1082]\ttraining's auc: 0.999943\tvalid_1's auc: 0.935854\n",
      "####################\n",
      "Global AUC 0.9358456321068517\n",
      "Week 70 0.9393707572621319 18970\n",
      "Week 71 0.9300599261268717 20726\n",
      "Week 72 0.9308680950699857 20332\n",
      "Week 73 0.9465869502523432 19010\n",
      "Week 74 0.9276225667469892 10288\n",
      "####################\n",
      "Features Preformance: 0.9358456321068517\n",
      "Diff with previous__: 0.0006803701267128481\n"
     ]
    }
   ],
   "source": [
    "########################### Mark card columns \"outliers\"\n",
    "print('Categorical outliers')\n",
    "## \n",
    "saved_state = list(full_df)\n",
    "####\n",
    "\n",
    "i_cols = ['TransactionAmt','ProductCD','P_emaildomain','R_emaildomain',]\n",
    "periods = ['DT_M']\n",
    "\n",
    "for period in periods:\n",
    "    for col in i_cols:\n",
    "        full_df[col+'_catboost_check_'+period] = full_df.groupby([col])[period].transform('nunique')\n",
    "        full_df[col+'_catboost_check_'+period] = np.where(full_df[col+'_catboost_check_'+period]==1,1,0).astype(np.int8)\n",
    "\n",
    "        \n",
    "####\n",
    "if MAKE_TESTS:\n",
    "    print(get_new_columns(saved_state))\n",
    "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D columns transformations\n",
      "   New columns (including dummy)        New Features\n",
      "0             D10_DT_D_std_score  D10_DT_D_std_score\n",
      "1             D11_DT_D_std_score  D11_DT_D_std_score\n",
      "2             D12_DT_D_std_score  D12_DT_D_std_score\n",
      "3             D13_DT_D_std_score  D13_DT_D_std_score\n",
      "4             D14_DT_D_std_score  D14_DT_D_std_score\n",
      "5             D15_DT_D_std_score  D15_DT_D_std_score\n",
      "6                      D1_scaled           D1_scaled\n",
      "7                      D2_scaled           D2_scaled\n",
      "8              D3_DT_D_std_score   D3_DT_D_std_score\n",
      "9              D4_DT_D_std_score   D4_DT_D_std_score\n",
      "10             D5_DT_D_std_score   D5_DT_D_std_score\n",
      "11             D6_DT_D_std_score   D6_DT_D_std_score\n",
      "12             D7_DT_D_std_score   D7_DT_D_std_score\n",
      "13             D8_DT_D_std_score   D8_DT_D_std_score\n",
      "14                     D9_scaled           D9_scaled\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.982164\tvalid_1's auc: 0.923619\n",
      "[400]\ttraining's auc: 0.995919\tvalid_1's auc: 0.932889\n",
      "[600]\ttraining's auc: 0.999063\tvalid_1's auc: 0.935108\n",
      "[800]\ttraining's auc: 0.999747\tvalid_1's auc: 0.935974\n",
      "[1000]\ttraining's auc: 0.999928\tvalid_1's auc: 0.936085\n",
      "Early stopping, best iteration is:\n",
      "[923]\ttraining's auc: 0.999883\tvalid_1's auc: 0.93629\n",
      "####################\n",
      "Global AUC 0.9362810421687262\n",
      "Week 70 0.9384867951383205 18970\n",
      "Week 71 0.9306896102502797 20726\n",
      "Week 72 0.9316554980444689 20332\n",
      "Week 73 0.9472702235039653 19010\n",
      "Week 74 0.928642649867649 10288\n",
      "####################\n",
      "Features Preformance: 0.9362810421687262\n",
      "Diff with previous__: 0.00043541006187453046\n"
     ]
    }
   ],
   "source": [
    "########################### D Columns Normalize and remove original columns\n",
    "print('D columns transformations')\n",
    "## \n",
    "saved_state = list(full_df)\n",
    "####\n",
    "\n",
    "# Remove original features\n",
    "# test data will have many unknow values\n",
    "i_cols = ['D'+str(i) for i in range(1,16)]\n",
    "remove_features += i_cols\n",
    "\n",
    "####### Values Normalization\n",
    "i_cols.remove('D1')\n",
    "i_cols.remove('D2')\n",
    "i_cols.remove('D9')\n",
    "periods = ['DT_D']\n",
    "for col in i_cols:\n",
    "    full_df[col] = full_df[col].clip(0)\n",
    "full_df = values_normalization(full_df, periods, i_cols, enc_type='norm')\n",
    "\n",
    "i_cols = ['D1','D2','D9']\n",
    "for col in i_cols:\n",
    "    full_df[col+'_scaled'] = full_df[col]/full_df[col].max()\n",
    "\n",
    "\n",
    "####\n",
    "if MAKE_TESTS:\n",
    "    print(get_new_columns(saved_state))\n",
    "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance normalization\n",
      "uid_td_D1_cUID_1\n",
      "card1\n",
      "  New columns (including dummy)                 New Features\n",
      "0              card1_dist1_norm             card1_dist1_norm\n",
      "1              card1_dist2_norm             card1_dist2_norm\n",
      "2   uid_td_D1_cUID_1_dist1_norm  uid_td_D1_cUID_1_dist1_norm\n",
      "3   uid_td_D1_cUID_1_dist2_norm  uid_td_D1_cUID_1_dist2_norm\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.981762\tvalid_1's auc: 0.922758\n",
      "[400]\ttraining's auc: 0.995941\tvalid_1's auc: 0.931719\n",
      "[600]\ttraining's auc: 0.999101\tvalid_1's auc: 0.934267\n",
      "[800]\ttraining's auc: 0.99976\tvalid_1's auc: 0.934997\n",
      "[1000]\ttraining's auc: 0.999933\tvalid_1's auc: 0.93547\n",
      "[1200]\ttraining's auc: 0.999981\tvalid_1's auc: 0.935623\n",
      "Early stopping, best iteration is:\n",
      "[1105]\ttraining's auc: 0.999966\tvalid_1's auc: 0.9357\n",
      "####################\n",
      "Global AUC 0.935704797669684\n",
      "Week 70 0.9382943044432832 18970\n",
      "Week 71 0.9293208883167041 20726\n",
      "Week 72 0.9298698694641729 20332\n",
      "Week 73 0.9469240086517665 19010\n",
      "Week 74 0.9302311114109151 10288\n",
      "####################\n",
      "Features Preformance: 0.935704797669684\n",
      "Diff with previous__: -0.0005762444990422555\n"
     ]
    }
   ],
   "source": [
    "########################### Dist\n",
    "print('Distance normalization')\n",
    "## \n",
    "saved_state = list(full_df)\n",
    "####\n",
    "\n",
    "i_cols = ['dist1','dist2']\n",
    "main_cols = [\n",
    "             'uid_td_D1_cUID_1',\n",
    "             'card1',\n",
    "            ]\n",
    "\n",
    "\n",
    "for main_col in main_cols:\n",
    "    print(main_col)\n",
    "    for agg_type in ['mean','std']:\n",
    "        temp_df = full_df.groupby([main_col])[i_cols].transform(agg_type)\n",
    "        temp_df.columns = [main_col+'_'+col+'_'+agg_type for col in list(temp_df)]\n",
    "        full_df = pd.concat([full_df,temp_df], axis=1)\n",
    "    \n",
    "    for col in i_cols:\n",
    "        f_std = main_col+'_'+col+'_std'\n",
    "        f_mean = main_col+'_'+col+'_mean'\n",
    "        full_df[main_col+'_'+col+'_norm'] = (full_df[col]-full_df[f_mean])/full_df[f_std]\n",
    "        del full_df[f_mean], full_df[f_std]\n",
    "\n",
    "\n",
    "####\n",
    "if MAKE_TESTS:\n",
    "    print(get_new_columns(saved_state))\n",
    "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar transactions per period\n",
      "        New columns (including dummy)                        New Features\n",
      "0  TransactionAmt_Product_counts_DT_D  TransactionAmt_Product_counts_DT_D\n",
      "1  TransactionAmt_Product_counts_DT_W  TransactionAmt_Product_counts_DT_W\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.982496\tvalid_1's auc: 0.923676\n",
      "[400]\ttraining's auc: 0.996205\tvalid_1's auc: 0.93274\n",
      "[600]\ttraining's auc: 0.999214\tvalid_1's auc: 0.935145\n",
      "[800]\ttraining's auc: 0.999793\tvalid_1's auc: 0.93598\n",
      "[1000]\ttraining's auc: 0.999945\tvalid_1's auc: 0.936426\n",
      "Early stopping, best iteration is:\n",
      "[999]\ttraining's auc: 0.999944\tvalid_1's auc: 0.936458\n",
      "####################\n",
      "Global AUC 0.936454245916349\n",
      "Week 70 0.9382882307077601 18970\n",
      "Week 71 0.9303240762027372 20726\n",
      "Week 72 0.9310705985620688 20332\n",
      "Week 73 0.9475900504686374 19010\n",
      "Week 74 0.9306203569416494 10288\n",
      "####################\n",
      "Features Preformance: 0.936454245916349\n",
      "Diff with previous__: 0.0007494482466650076\n"
     ]
    }
   ],
   "source": [
    "########################### Count similar transactions per period\n",
    "print('Similar transactions per period')\n",
    "## \n",
    "saved_state = list(full_df)\n",
    "####\n",
    "\n",
    "periods = ['DT_W','DT_D'] \n",
    "\n",
    "for period in periods:\n",
    "    full_df['TransactionAmt_Product_counts_' + period] =\\\n",
    "        full_df.groupby([period,'ProductCD','TransactionAmt'])['TransactionAmt'].transform('count')\n",
    "    full_df['TransactionAmt_Product_counts_' + period] /= full_df[period+'_total']\n",
    "\n",
    "####\n",
    "if MAKE_TESTS:\n",
    "    print(get_new_columns(saved_state))\n",
    "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nunique dates per client\n",
      "                 New columns (including dummy)  \\\n",
      "0           D8_catboost_check_uid_td_D1_cUID_1   \n",
      "1           D9_catboost_check_uid_td_D1_cUID_1   \n",
      "2   uid_td_D10_catboost_check_uid_td_D1_cUID_1   \n",
      "3   uid_td_D11_catboost_check_uid_td_D1_cUID_1   \n",
      "4   uid_td_D12_catboost_check_uid_td_D1_cUID_1   \n",
      "5   uid_td_D13_catboost_check_uid_td_D1_cUID_1   \n",
      "6   uid_td_D14_catboost_check_uid_td_D1_cUID_1   \n",
      "7   uid_td_D15_catboost_check_uid_td_D1_cUID_1   \n",
      "8    uid_td_D2_catboost_check_uid_td_D1_cUID_1   \n",
      "9    uid_td_D3_catboost_check_uid_td_D1_cUID_1   \n",
      "10   uid_td_D4_catboost_check_uid_td_D1_cUID_1   \n",
      "11   uid_td_D5_catboost_check_uid_td_D1_cUID_1   \n",
      "12   uid_td_D6_catboost_check_uid_td_D1_cUID_1   \n",
      "13   uid_td_D7_catboost_check_uid_td_D1_cUID_1   \n",
      "14   uid_td_D8_catboost_check_uid_td_D1_cUID_1   \n",
      "\n",
      "                                  New Features  \n",
      "0           D8_catboost_check_uid_td_D1_cUID_1  \n",
      "1           D9_catboost_check_uid_td_D1_cUID_1  \n",
      "2   uid_td_D10_catboost_check_uid_td_D1_cUID_1  \n",
      "3   uid_td_D11_catboost_check_uid_td_D1_cUID_1  \n",
      "4   uid_td_D12_catboost_check_uid_td_D1_cUID_1  \n",
      "5   uid_td_D13_catboost_check_uid_td_D1_cUID_1  \n",
      "6   uid_td_D14_catboost_check_uid_td_D1_cUID_1  \n",
      "7   uid_td_D15_catboost_check_uid_td_D1_cUID_1  \n",
      "8    uid_td_D2_catboost_check_uid_td_D1_cUID_1  \n",
      "9    uid_td_D3_catboost_check_uid_td_D1_cUID_1  \n",
      "10   uid_td_D4_catboost_check_uid_td_D1_cUID_1  \n",
      "11   uid_td_D5_catboost_check_uid_td_D1_cUID_1  \n",
      "12   uid_td_D6_catboost_check_uid_td_D1_cUID_1  \n",
      "13   uid_td_D7_catboost_check_uid_td_D1_cUID_1  \n",
      "14   uid_td_D8_catboost_check_uid_td_D1_cUID_1  \n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.982879\tvalid_1's auc: 0.925651\n",
      "[400]\ttraining's auc: 0.996219\tvalid_1's auc: 0.934275\n",
      "[600]\ttraining's auc: 0.999226\tvalid_1's auc: 0.937011\n",
      "[800]\ttraining's auc: 0.999802\tvalid_1's auc: 0.937524\n",
      "Early stopping, best iteration is:\n",
      "[750]\ttraining's auc: 0.999725\tvalid_1's auc: 0.937742\n",
      "####################\n",
      "Global AUC 0.9377394677117579\n",
      "Week 70 0.9393761769030602 18970\n",
      "Week 71 0.9308695418946467 20726\n",
      "Week 72 0.9345440169568234 20332\n",
      "Week 73 0.9485346791636626 19010\n",
      "Week 74 0.9301156868697089 10288\n",
      "####################\n",
      "Features Preformance: 0.9377394677117579\n",
      "Diff with previous__: 0.0012852217954089085\n"
     ]
    }
   ],
   "source": [
    "########################### Find nunique dates per client\n",
    "print('Nunique dates per client')\n",
    "## \n",
    "saved_state = list(full_df)\n",
    "####\n",
    "\n",
    "main_cols = {\n",
    "            'uid_td_D1_cUID_1': ['uid_td_D'+str(i) for i in range(2,16) if i!=9] + ['D8','D9'],\n",
    "            }\n",
    "\n",
    "for main_col,i_cols in main_cols.items():\n",
    "    for col in i_cols:\n",
    "        full_df[col+'_catboost_check_'+main_col] = full_df.groupby([main_col])[col].transform('nunique')\n",
    "\n",
    "####\n",
    "if MAKE_TESTS:\n",
    "    print(get_new_columns(saved_state))\n",
    "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email split\n",
      "  New columns (including dummy)       New Features\n",
      "0                email_p_domain     email_p_domain\n",
      "1             email_p_extension  email_p_extension\n",
      "2                email_r_domain     email_r_domain\n",
      "3             email_r_extension  email_r_extension\n",
      "4                    full_email         full_email\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.982241\tvalid_1's auc: 0.925386\n",
      "[400]\ttraining's auc: 0.996285\tvalid_1's auc: 0.933921\n",
      "[600]\ttraining's auc: 0.999172\tvalid_1's auc: 0.935761\n",
      "[800]\ttraining's auc: 0.999767\tvalid_1's auc: 0.936388\n",
      "Early stopping, best iteration is:\n",
      "[868]\ttraining's auc: 0.999846\tvalid_1's auc: 0.936694\n",
      "####################\n",
      "Global AUC 0.9367011131258307\n",
      "Week 70 0.9385187523313799 18970\n",
      "Week 71 0.9307852468312718 20726\n",
      "Week 72 0.9330384660644889 20332\n",
      "Week 73 0.9477682768565249 19010\n",
      "Week 74 0.9277387407982031 10288\n",
      "####################\n",
      "Features Preformance: 0.9367011131258307\n",
      "Diff with previous__: -0.0010383545859271592\n"
     ]
    }
   ],
   "source": [
    "########################### Email transformation\n",
    "print('Email split')\n",
    "saved_state = list(full_df)\n",
    "####\n",
    "\n",
    "p = 'P_emaildomain'\n",
    "r = 'R_emaildomain'\n",
    "\n",
    "full_df['full_email'] = full_df[p].astype(str) +'_'+ full_df[r].astype(str)\n",
    "full_df['email_p_extension'] = full_df[p].apply(lambda x: str(x).split('.')[-1])\n",
    "full_df['email_r_extension'] = full_df[r].apply(lambda x: str(x).split('.')[-1])\n",
    "full_df['email_p_domain'] = full_df[p].apply(lambda x: str(x).split('.')[0])\n",
    "full_df['email_r_domain'] = full_df[r].apply(lambda x: str(x).split('.')[0])\n",
    "\n",
    "i_cols = ['P_emaildomain','R_emaildomain',\n",
    "          'full_email',\n",
    "          'email_p_extension','email_r_extension',\n",
    "          'email_p_domain','email_r_domain']\n",
    "\n",
    "full_df = frequency_encoding(full_df, i_cols, self_encoding=True)\n",
    "\n",
    "####\n",
    "if MAKE_TESTS:\n",
    "    print(get_new_columns(saved_state))\n",
    "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identity sets\n",
      "   New columns (including dummy)        New Features\n",
      "0                     DeviceInfo          DeviceInfo\n",
      "1              DeviceInfo_device   DeviceInfo_device\n",
      "2             DeviceInfo_version  DeviceInfo_version\n",
      "3                     DeviceType          DeviceType\n",
      "4                          id_01               id_01\n",
      "5                          id_02               id_02\n",
      "6                          id_03               id_03\n",
      "7                          id_04               id_04\n",
      "8                          id_05               id_05\n",
      "9                          id_06               id_06\n",
      "10                         id_07               id_07\n",
      "11                         id_08               id_08\n",
      "12                         id_09               id_09\n",
      "13                         id_10               id_10\n",
      "14                         id_11               id_11\n",
      "15                         id_12               id_12\n",
      "16                         id_13               id_13\n",
      "17                         id_14               id_14\n",
      "18                         id_15               id_15\n",
      "19                         id_16               id_16\n",
      "20                         id_17               id_17\n",
      "21                         id_18               id_18\n",
      "22                         id_19               id_19\n",
      "23                         id_20               id_20\n",
      "24                         id_21               id_21\n",
      "25                         id_22               id_22\n",
      "26                         id_23               id_23\n",
      "27                         id_24               id_24\n",
      "28                         id_25               id_25\n",
      "29                         id_26               id_26\n",
      "30                         id_27               id_27\n",
      "31                         id_28               id_28\n",
      "32                         id_29               id_29\n",
      "33                         id_30               id_30\n",
      "34                  id_30_device        id_30_device\n",
      "35                 id_30_version       id_30_version\n",
      "36                         id_31               id_31\n",
      "37                  id_31_device        id_31_device\n",
      "38                         id_32               id_32\n",
      "39                         id_33               id_33\n",
      "40                       id_33_0             id_33_0\n",
      "41                       id_33_1             id_33_1\n",
      "42                         id_34               id_34\n",
      "43                         id_35               id_35\n",
      "44                         id_36               id_36\n",
      "45                         id_37               id_37\n",
      "46                         id_38               id_38\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.982808\tvalid_1's auc: 0.924687\n",
      "[400]\ttraining's auc: 0.996668\tvalid_1's auc: 0.933955\n",
      "[600]\ttraining's auc: 0.999388\tvalid_1's auc: 0.936411\n",
      "[800]\ttraining's auc: 0.999855\tvalid_1's auc: 0.937232\n",
      "[1000]\ttraining's auc: 0.999961\tvalid_1's auc: 0.937703\n",
      "Early stopping, best iteration is:\n",
      "[977]\ttraining's auc: 0.999955\tvalid_1's auc: 0.937731\n",
      "####################\n",
      "Global AUC 0.9377247357643645\n",
      "Week 70 0.9402150999419165 18970\n",
      "Week 71 0.9342746792955998 20726\n",
      "Week 72 0.9311248507265558 20332\n",
      "Week 73 0.9484293439077145 19010\n",
      "Week 74 0.9299080725975395 10288\n",
      "####################\n",
      "Features Preformance: 0.9377247357643645\n",
      "Diff with previous__: 0.0010236226385338387\n"
     ]
    }
   ],
   "source": [
    "########################### Device info and identity\n",
    "print('Identity sets')\n",
    "saved_state = list(full_df)\n",
    "####\n",
    "\n",
    "########################### Device info\n",
    "identity_df['DeviceInfo'] = identity_df['DeviceInfo'].fillna('unknown_device').str.lower()\n",
    "identity_df['DeviceInfo_device'] = identity_df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
    "identity_df['DeviceInfo_version'] = identity_df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n",
    "    \n",
    "########################### Device info 2\n",
    "identity_df['id_30'] = identity_df['id_30'].fillna('unknown_device').str.lower()\n",
    "identity_df['id_30_device'] = identity_df['id_30'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
    "identity_df['id_30_version'] = identity_df['id_30'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n",
    "    \n",
    "########################### Browser\n",
    "identity_df['id_31'] = identity_df['id_31'].fillna('unknown_device').str.lower()\n",
    "identity_df['id_31_device'] = identity_df['id_31'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
    "    \n",
    "########################### Merge Identity columns\n",
    "temp_df = full_df[['TransactionID']]\n",
    "temp_df = temp_df.merge(identity_df, on=['TransactionID'], how='left')\n",
    "del temp_df['TransactionID']\n",
    "full_df = pd.concat([full_df,temp_df], axis=1)\n",
    "  \n",
    "i_cols = [\n",
    "          'DeviceInfo','DeviceInfo_device','DeviceInfo_version',\n",
    "          'id_30','id_30_device','id_30_version',\n",
    "          'id_31','id_31_device',\n",
    "          'id_33','DeviceType'\n",
    "         ]\n",
    "\n",
    "####### Global Self frequency encoding\n",
    "full_df = frequency_encoding(full_df, i_cols, self_encoding=True)\n",
    "\n",
    "####\n",
    "if MAKE_TESTS:\n",
    "    print(get_new_columns(saved_state))\n",
    "    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Export\n",
    "full_df.to_pickle('baseline_full_df.pkl')\n",
    "\n",
    "remove_features_df = pd.DataFrame(remove_features, columns=['features_to_remove'])\n",
    "remove_features_df.to_pickle('baseline_remove_features.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
