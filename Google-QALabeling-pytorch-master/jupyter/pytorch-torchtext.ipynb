{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libs and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os, random, sys, time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext\n",
    "from torchtext import vocab, data\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../input/glove-reddit-comments/')\n",
    "from clean_text import RegExCleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../input/google-quest-challenge'\n",
    "EMB_PATH = '../input/embeddings-glove-crawl-torch-cached'\n",
    "EMB_FILENAME = 'GloVe.Reddit.120B.512D.txt'\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "N_SPLITS = 4\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\n",
    "subm = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'), index_col='qa_id')\n",
    "subm.loc[:, :] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just simple tokenizer\n",
    "tknz = RegExCleaner.reddits()\n",
    "def tokenizer(text):\n",
    "    text = tknz(text).split()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the columns that we want to process and how to process\n",
    "\n",
    "## text field, We have 2 text field, so we must fixed by length\n",
    "txt_field = data.Field(sequential=True, tokenize=tokenizer, fix_length=125,\n",
    "                       batch_first=False, include_lengths=False,  use_vocab=True)\n",
    "## Numeric fields\n",
    "num_field = data.Field(sequential=False, dtype=torch.float,  use_vocab=False)\n",
    "idx_field = data.Field(sequential=False, dtype=torch.int,  use_vocab=False)\n",
    "## Fields which don't need preprocessing\n",
    "raw_field = data.RawField()\n",
    "\n",
    "labels = df.columns[11:]\n",
    "num_columns = list(zip(labels, [num_field]*len(labels)))\n",
    "basic_columns = [\n",
    "    ('qa_id', idx_field),\n",
    "    ('question_title', raw_field),\n",
    "    ('question_body', txt_field),\n",
    "    ('question_user_name', raw_field),\n",
    "    ('question_user_page', raw_field),\n",
    "    ('answer', txt_field),\n",
    "    ('answer_user_name', raw_field),\n",
    "    ('answer_user_page', raw_field),\n",
    "    ('url', raw_field),\n",
    "    ('category', raw_field),\n",
    "    ('host', raw_field),\n",
    "]\n",
    "train_fields = basic_columns + num_columns\n",
    "test_fields = basic_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading csv file\n",
    "train_ds = data.TabularDataset(path=os.path.join(DATA_PATH, 'train.csv'), \n",
    "                           format='csv',\n",
    "                           fields=train_fields, \n",
    "                           skip_header=True)\n",
    "\n",
    "test_ds = data.TabularDataset(path=os.path.join(DATA_PATH, 'test.csv'), \n",
    "                           format='csv',\n",
    "                           fields=test_fields, \n",
    "                           skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk of answer: It might be helpful to look into the definition of\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "print('Chunk of answer: '+' '.join(train_ds.examples[1].answer[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vocab size:  59690\n"
     ]
    }
   ],
   "source": [
    "# specify the path to the localy saved vectors\n",
    "vec = vocab.Vectors(os.path.join(EMB_PATH, EMB_FILENAME), cache=EMB_PATH)\n",
    "# build the vocabulary using train and validation dataset and assign the vectors\n",
    "txt_field.build_vocab(train_ds, test_ds, max_size=300000, vectors=vec)\n",
    "\n",
    "embs_vocab = train_ds.fields['question_body'].vocab.vectors\n",
    "print('Embedding vocab size: ', embs_vocab.size()[0])\n",
    "vocab_size = embs_vocab.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper for loaders, which structured fields\n",
    "class BatchWrapper:\n",
    "      def __init__(self, dataloader, mode='train'):\n",
    "            self.dataloader, self.mode = dataloader, mode\n",
    "     \n",
    "      def __iter__(self):\n",
    "            if self.mode =='test':\n",
    "                for batch in self.dataloader:\n",
    "                    yield (batch.qa_id, batch.question_body, batch.answer)\n",
    "            else:\n",
    "                for batch in self.dataloader:\n",
    "                    target = torch.stack([getattr(batch, label) for label in labels], dim=-1)\n",
    "                    yield (batch.question_body,  batch.answer, target)\n",
    "  \n",
    "      def __len__(self):\n",
    "            return len(self.dl)\n",
    "\n",
    "def wrapper(ds, mode='train', **kwargs):\n",
    "    dataloader = data.BucketIterator(ds, device=DEVICE, **kwargs)\n",
    "    return BatchWrapper(dataloader, mode)\n",
    "\n",
    "def splits_cv(dataset, cv, y=None, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "        Split dataset to train and validation used cross-validator and wrap loader\n",
    "    \"\"\"\n",
    "    for indices in cv.split(range(len(dataset)), y):\n",
    "        (train_data, valid_data) = tuple([dataset.examples[i] for i in index] for index in indices)\n",
    "        yield tuple(wrapper(data.Dataset(d, dataset.fields), batch_size=batch_size) for d in (train_data, valid_data) if d)\n",
    "        \n",
    "cv = KFold(n_splits=N_SPLITS, random_state=6699)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = wrapper(test_ds, batch_size=BATCH_SIZE, shuffle=False, repeat=False, mode='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_QA(nn.Module):\n",
    "    def __init__(self, embs_vocab, hidden_size=64, layers=1,\n",
    "                 dropout=0., bidirectional=False, num_classes=30):\n",
    "        super().__init__()\n",
    "\n",
    "        coef = 2 if bidirectional else 1\n",
    "        dropout = dropout if layers > 1 else 0\n",
    "        self.emb = nn.Embedding.from_pretrained(embs_vocab, freeze=True)\n",
    "                \n",
    "        self.question = nn.LSTM(embs_vocab.size(1), hidden_size,\n",
    "                            num_layers=layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        \n",
    "        self.answer = nn.LSTM(embs_vocab.size(1), hidden_size,\n",
    "                            num_layers=layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "                nn.Linear(2*hidden_size*coef, 64),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(64, num_classes)\n",
    "            )\n",
    "                \n",
    "    def forward(self, q, a):\n",
    "        \n",
    "        q = self.emb(q)\n",
    "        a = self.emb(a)\n",
    "        \n",
    "        q_rnn, _ = self.question(q)\n",
    "        a_rnn, _ = self.answer(a)\n",
    "        \n",
    "        q_rnn, _ = q_rnn.max(dim=0, keepdim=False) \n",
    "        a_rnn, _ = a_rnn.max(dim=0, keepdim=False) \n",
    "        \n",
    "        out = torch.cat([q_rnn, a_rnn], dim=-1)\n",
    "        out = self.classifier(out).sigmoid()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, oof prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_fn(p, t):\n",
    "    score = 0\n",
    "    for i in range(p.shape[1]):\n",
    "        score += np.nan_to_num(spearmanr(p[:,i], t[:,i])[0])\n",
    "    score /= 30\n",
    "    return score\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation_fn(model, loader, loss_fn):\n",
    "    y_pred, y_true, tloss = [], [], []\n",
    "    for q, a, target in loader:\n",
    "        outputs = model(q, a)\n",
    "        loss = loss_fn(outputs, target)\n",
    "        tloss.append(loss.item())\n",
    "        y_true.append(target.detach().cpu().numpy())\n",
    "        y_pred.append(outputs.detach().cpu().numpy())\n",
    "        \n",
    "    tloss = np.array(tloss).mean()\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    y_true = np.concatenate(y_true)\n",
    "    metric = metric_fn(y_pred, y_true)\n",
    "    return tloss, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Table for results\n",
    "header = r'''\n",
    "           Train       Validation\n",
    "Epoch | Loss |Spearm| Loss |Spearm| Time, m\n",
    "'''\n",
    "#          Epoch         metrics            time\n",
    "raw_line = '{:6d}' + '\\u2502{:6.3f}'*4 + '\\u2502{:6.2f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oof_preds(train_ds, test_loader, embs_vocab, epochs = 4):\n",
    "\n",
    "    for loader, vloader in splits_cv(train_ds, cv):\n",
    "        \n",
    "        model = RNN_QA(embs_vocab, hidden_size=128, dropout=0.1, bidirectional=True).to(DEVICE)\n",
    "        \n",
    "        optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), 1e-3,\n",
    "                                                     betas=(0.75, 0.999), weight_decay=1e-3)\n",
    "        loss_fn = torch.nn.BCELoss()\n",
    "        print(header)\n",
    "        for epoch in range(1,epochs+1):      \n",
    "            y_pred, y_true = [], []\n",
    "            start_time = time.time()\n",
    "            tloss = []          \n",
    "            model.train()\n",
    "            \n",
    "            for q, a, target in loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(q, a)\n",
    "                loss = loss_fn(outputs, target)\n",
    "                tloss.append(loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                y_true.append(target.detach().cpu().numpy())\n",
    "                y_pred.append(outputs.detach().cpu().numpy())\n",
    "\n",
    "            tloss = np.array(tloss).mean()\n",
    "            y_pred = np.concatenate(y_pred)\n",
    "            y_true = np.concatenate(y_true)\n",
    "            tmetric = metric_fn(y_pred, y_true)\n",
    "\n",
    "            vloss, vmetric = validation_fn(model, vloader, loss_fn)\n",
    "            if epoch % 2 == 0:\n",
    "                print(raw_line.format(epoch,tloss,tmetric,vloss,vmetric,(time.time()-start_time)/60**1))\n",
    "\n",
    "       \n",
    "        # Get prediction for test set\n",
    "        qa_id, preds = [], [] \n",
    "        with torch.no_grad():\n",
    "            for qaids, q, a in test_loader:\n",
    "                outputs = model(q, a)\n",
    "                qa_id.append(qaids.cpu().numpy())\n",
    "                preds.append(outputs.detach().cpu().numpy())\n",
    "            \n",
    "        # Save prediction of test set\n",
    "        qa_id = np.concatenate(qa_id)\n",
    "        preds = np.concatenate(preds)\n",
    "        subm.loc[qa_id, labels]  =  subm.loc[qa_id, labels].values + preds / N_SPLITS\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "           Train       Validation\n",
      "Epoch | Loss |Spearm| Loss |Spearm| Time, m\n",
      "\n",
      "     2│ 0.414│ 0.137│ 0.411│ 0.201│  0.04\n",
      "     4│ 0.401│ 0.246│ 0.404│ 0.258│  0.04\n",
      "     6│ 0.394│ 0.277│ 0.399│ 0.277│  0.04\n",
      "     8│ 0.387│ 0.306│ 0.395│ 0.290│  0.04\n",
      "    10│ 0.380│ 0.336│ 0.393│ 0.301│  0.04\n",
      "    12│ 0.371│ 0.364│ 0.391│ 0.307│  0.04\n",
      "    14│ 0.363│ 0.393│ 0.391│ 0.311│  0.04\n",
      "    16│ 0.354│ 0.418│ 0.393│ 0.315│  0.04\n",
      "    18│ 0.346│ 0.444│ 0.391│ 0.316│  0.04\n",
      "    20│ 0.339│ 0.462│ 0.396│ 0.316│  0.04\n",
      "\n",
      "           Train       Validation\n",
      "Epoch | Loss |Spearm| Loss |Spearm| Time, m\n",
      "\n",
      "     2│ 0.414│ 0.178│ 0.408│ 0.226│  0.04\n",
      "     4│ 0.403│ 0.235│ 0.402│ 0.248│  0.04\n",
      "     6│ 0.397│ 0.270│ 0.396│ 0.273│  0.04\n",
      "     8│ 0.388│ 0.313│ 0.394│ 0.294│  0.04\n",
      "    10│ 0.379│ 0.347│ 0.389│ 0.304│  0.04\n",
      "    12│ 0.370│ 0.376│ 0.388│ 0.310│  0.04\n",
      "    14│ 0.360│ 0.410│ 0.389│ 0.313│  0.04\n",
      "    16│ 0.351│ 0.436│ 0.390│ 0.314│  0.04\n",
      "    18│ 0.341│ 0.462│ 0.390│ 0.315│  0.04\n",
      "    20│ 0.333│ 0.484│ 0.394│ 0.315│  0.04\n",
      "\n",
      "           Train       Validation\n",
      "Epoch | Loss |Spearm| Loss |Spearm| Time, m\n",
      "\n",
      "     2│ 0.417│ 0.136│ 0.410│ 0.183│  0.04\n",
      "     4│ 0.404│ 0.220│ 0.402│ 0.242│  0.04\n",
      "     6│ 0.398│ 0.261│ 0.398│ 0.269│  0.04\n",
      "     8│ 0.391│ 0.295│ 0.395│ 0.285│  0.04\n",
      "    10│ 0.383│ 0.328│ 0.393│ 0.298│  0.04\n",
      "    12│ 0.375│ 0.353│ 0.389│ 0.308│  0.04\n",
      "    14│ 0.367│ 0.383│ 0.388│ 0.312│  0.04\n",
      "    16│ 0.358│ 0.408│ 0.388│ 0.316│  0.04\n",
      "    18│ 0.350│ 0.436│ 0.391│ 0.320│  0.04\n",
      "    20│ 0.342│ 0.459│ 0.394│ 0.323│  0.04\n",
      "\n",
      "           Train       Validation\n",
      "Epoch | Loss |Spearm| Loss |Spearm| Time, m\n",
      "\n",
      "     2│ 0.416│ 0.147│ 0.412│ 0.193│  0.04\n",
      "     4│ 0.403│ 0.232│ 0.404│ 0.234│  0.04\n",
      "     6│ 0.396│ 0.266│ 0.399│ 0.256│  0.04\n",
      "     8│ 0.390│ 0.300│ 0.395│ 0.280│  0.04\n",
      "    10│ 0.382│ 0.336│ 0.393│ 0.295│  0.04\n",
      "    12│ 0.374│ 0.367│ 0.390│ 0.302│  0.04\n",
      "    14│ 0.366│ 0.393│ 0.390│ 0.308│  0.04\n",
      "    16│ 0.357│ 0.421│ 0.390│ 0.309│  0.04\n",
      "    18│ 0.348│ 0.446│ 0.389│ 0.311│  0.04\n",
      "    20│ 0.340│ 0.469│ 0.390│ 0.314│  0.04\n"
     ]
    }
   ],
   "source": [
    "oof_preds(train_ds, test_loader, embs_vocab, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
