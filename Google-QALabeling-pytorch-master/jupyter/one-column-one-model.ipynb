{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/labdmitriy/baseline-linear\n",
    "# https://www.kaggle.com/tunguz/quest-simple-eda\n",
    "\n",
    "# https://www.kaggle.com/abhishek/distilbert-use-features-oof\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ../input/sacremoses/sacremoses-master/ > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, \"../input/transformers/transformers-master/\")\n",
    "import transformers\n",
    "import math\n",
    "\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import pickle  \n",
    "import random\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "from scipy.stats import spearmanr, rankdata\n",
    "from os.path import join as path_join\n",
    "from numpy.random import seed\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# import category_encoders as ce\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, OneHotEncoder, RobustScaler, KBinsDiscretizer, QuantileTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, KFold, GroupKFold\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, HuberRegressor, RANSACRegressor\n",
    "from sklearn.svm import LinearSVR, SVR\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "data_dir = '../input/google-quest-challenge/'\n",
    "metas_dir = ''\n",
    "sub_dir = ''\n",
    "\n",
    "# data_dir = '../input/'\n",
    "# metas_dir = '../metafeatures/'\n",
    "# sub_dir = '../submissions/'\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "import datetime\n",
    "todate = datetime.date.today().strftime(\"%m%d\")\n",
    "\n",
    "\n",
    "nfolds = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count words\n",
    "def word_count(xstring):\n",
    "    return xstring.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearman_corr(y_true, y_pred):\n",
    "        if np.ndim(y_pred) == 2:\n",
    "            corr = np.mean([stats.spearmanr(y_true[:, i], y_pred[:, i])[0] for i in range(y_true.shape[1])])\n",
    "        else:\n",
    "            corr = stats.spearmanr(y_true, y_pred)[0]\n",
    "        return corr\n",
    "    \n",
    "custom_scorer = make_scorer(spearman_corr, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_vectors(string_list, batch_size=64):\n",
    "    # inspired by https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    tokenizer = transformers.DistilBertTokenizer.from_pretrained(\"../input/distilbertbaseuncased/\")\n",
    "    model = transformers.DistilBertModel.from_pretrained(\"../input/distilbertbaseuncased/\")\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    fin_features = []\n",
    "    for data in chunks(string_list, batch_size):\n",
    "        tokenized = []\n",
    "        for x in data:\n",
    "            x = \" \".join(x.strip().split()[:300])\n",
    "            tok = tokenizer.encode(x, add_special_tokens=True)\n",
    "            tokenized.append(tok[:512])\n",
    "\n",
    "        max_len = 512\n",
    "        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n",
    "        attention_mask = np.where(padded != 0, 1, 0)\n",
    "        input_ids = torch.tensor(padded).to(DEVICE)\n",
    "        attention_mask = torch.tensor(attention_mask).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        features = last_hidden_states[0][:, 0, :].cpu().numpy()\n",
    "        fin_features.append(features)\n",
    "\n",
    "    fin_features = np.vstack(fin_features)\n",
    "    return fin_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "xtrain = pd.read_csv(data_dir + 'train.csv')\n",
    "xtest = pd.read_csv(data_dir + 'test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['question_asker_intent_understanding', 'question_body_critical', \n",
    "               'question_conversational', 'question_expect_short_answer', \n",
    "               'question_fact_seeking', 'question_has_commonly_accepted_answer', \n",
    "               'question_interestingness_others', 'question_interestingness_self', \n",
    "               'question_multi_intent', 'question_not_really_a_question', \n",
    "               'question_opinion_seeking', 'question_type_choice', \n",
    "               'question_type_compare', 'question_type_consequence', \n",
    "               'question_type_definition', 'question_type_entity', \n",
    "               'question_type_instructions', 'question_type_procedure', \n",
    "               'question_type_reason_explanation', 'question_type_spelling', \n",
    "               'question_well_written', 'answer_helpful', \n",
    "               'answer_level_of_information', 'answer_plausible', \n",
    "               'answer_relevance', 'answer_satisfaction', \n",
    "               'answer_type_instructions', 'answer_type_procedure', \n",
    "               'answer_type_reason_explanation', 'answer_well_written']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA / FE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic FE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word count in title, body and answer\n",
    "for colname in ['question_title', 'question_body', 'answer']:\n",
    "    newname = colname + '_word_len'\n",
    "    \n",
    "    xtrain[newname] = xtrain[colname].str.split().str.len()\n",
    "    xtest[newname] = xtest[colname].str.split().str.len()\n",
    "\n",
    "    \n",
    "del newname, colname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for colname in ['question', 'answer']:\n",
    "\n",
    "    # check for nonames, i.e. users with logins like user12389\n",
    "    xtrain['is_'+colname+'_no_name_user'] = xtrain[colname +'_user_name'].str.contains('^user\\d+$') + 0\n",
    "    xtest['is_'+colname+'_no_name_user'] = xtest[colname +'_user_name'].str.contains('^user\\d+$') + 0\n",
    "    \n",
    "\n",
    "colname = 'answer'\n",
    "# check lexical diversity (unique words count vs total )\n",
    "xtrain[colname+'_div'] = xtrain[colname].apply(lambda s: len(set(s.split())) / len(s.split()) )\n",
    "xtest[colname+'_div'] = xtest[colname].apply(lambda s: len(set(s.split())) / len(s.split()) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## domain components\n",
    "xtrain['domcom'] = xtrain['question_user_page'].apply(lambda s: s.split('://')[1].split('/')[0].split('.'))\n",
    "xtest['domcom'] = xtest['question_user_page'].apply(lambda s: s.split('://')[1].split('/')[0].split('.'))\n",
    "\n",
    "# count components\n",
    "xtrain['dom_cnt'] = xtrain['domcom'].apply(lambda s: len(s))\n",
    "xtest['dom_cnt'] = xtest['domcom'].apply(lambda s: len(s))\n",
    "\n",
    "# extend length\n",
    "xtrain['domcom'] = xtrain['domcom'].apply(lambda s: s + ['none', 'none'])\n",
    "xtest['domcom'] = xtest['domcom'].apply(lambda s: s + ['none', 'none'])\n",
    "\n",
    "# components\n",
    "for ii in range(0,4):\n",
    "    xtrain['dom_'+str(ii)] = xtrain['domcom'].apply(lambda s: s[ii])\n",
    "    xtest['dom_'+str(ii)] = xtest['domcom'].apply(lambda s: s[ii])\n",
    "    \n",
    "# clean up\n",
    "xtrain.drop('domcom', axis = 1, inplace = True)\n",
    "xtest.drop('domcom', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared elements\n",
    "xtrain['q_words'] = xtrain['question_body'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords] )\n",
    "xtrain['a_words'] = xtrain['answer'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords] )\n",
    "xtrain['qa_word_overlap'] = xtrain.apply(lambda s: len(np.intersect1d(s['q_words'], s['a_words'])), axis = 1)\n",
    "xtrain['qa_word_overlap_norm1'] = xtrain.apply(lambda s: s['qa_word_overlap']/(1 + len(s['a_words'])), axis = 1)\n",
    "xtrain['qa_word_overlap_norm2'] = xtrain.apply(lambda s: s['qa_word_overlap']/(1 + len(s['q_words'])), axis = 1)\n",
    "\n",
    "xtest['q_words'] = xtest['question_body'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords] )\n",
    "xtest['a_words'] = xtest['answer'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords] )\n",
    "xtest['qa_word_overlap'] = xtest.apply(lambda s: len(np.intersect1d(s['q_words'], s['a_words'])), axis = 1)\n",
    "xtest['qa_word_overlap_norm1'] = xtest.apply(lambda s: s['qa_word_overlap']/(1 + len(s['a_words'])), axis = 1)\n",
    "xtest['qa_word_overlap_norm2'] = xtest.apply(lambda s: s['qa_word_overlap']/(1 + len(s['q_words'])), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of characters in the text ##\n",
    "xtrain[\"question_body_num_chars\"] = xtrain[\"question_body\"].apply(lambda x: len(str(x)))\n",
    "xtest[\"question_body_num_chars\"] = xtest[\"question_body\"].apply(lambda x: len(str(x)))\n",
    "xtrain[\"answer_num_chars\"] = xtrain[\"answer\"].apply(lambda x: len(str(x)))\n",
    "xtest[\"answer_num_chars\"] = xtest[\"answer\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "## Number of stopwords in the text ##\n",
    "xtrain[\"question_body_num_stopwords\"] = xtrain[\"question_body\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "xtest[\"question_body_num_stopwords\"] = xtest[\"question_body\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "xtrain[\"answer_num_stopwords\"] = xtrain[\"answer\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "xtest[\"answer_num_stopwords\"] = xtest[\"answer\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "xtrain['question_prop_stopwords'] = xtrain['question_body_num_stopwords'] / (xtrain['q_words'].apply(len))\n",
    "xtest['question_prop_stopwords'] = xtest['question_body_num_stopwords'] / (xtest['q_words'].apply(len))\n",
    "\n",
    "xtrain['answer_prop_stopwords'] = xtrain['answer_num_stopwords'] /( xtrain['a_words'].apply(len))\n",
    "xtest['answer_prop_stopwords'] = xtest['answer_num_stopwords'] / (xtest['a_words'].apply(len))\n",
    "\n",
    "## Number of punctuations in the text ##\n",
    "xtrain[\"question_body_num_punctuations\"] =xtrain['question_body'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "xtest[\"question_body_num_punctuations\"] =xtest['question_body'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "xtrain[\"answer_num_punctuations\"] =xtrain['answer'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "xtest[\"answer_num_punctuations\"] =xtest['answer'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "xtrain['question_prop_punctuations'] = xtrain['question_body_num_punctuations'] / xtrain['question_body_num_chars']\n",
    "xtest['question_prop_punctuations'] = xtest['question_body_num_punctuations'] / xtest['question_body_num_chars']\n",
    "\n",
    "xtrain['answer_prop_punctuations'] = xtrain['answer_num_punctuations'] / xtrain['answer_num_chars']\n",
    "xtest['answer_prop_punctuations'] = xtest['answer_num_punctuations'] / xtest['answer_num_chars']\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "xtrain[\"question_body_num_words_upper\"] = xtrain[\"question_body\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "xtest[\"question_body_num_words_upper\"] = xtest[\"question_body\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "xtrain[\"answer_num_words_upper\"] = xtrain[\"answer\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "xtest[\"answer_num_words_upper\"] = xtest[\"answer\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "\n",
    "xtrain['question_prop_upper'] = xtrain['question_body_num_words_upper'] / (xtrain['q_words'].apply(len))\n",
    "xtest['question_prop_upper'] = xtest['question_body_num_words_upper'] / (xtest['q_words'].apply(len))\n",
    "xtrain['answer_prop_upper'] = xtrain['answer_num_words_upper'] / (xtrain['a_words'].apply(len))\n",
    "xtest['answer_prop_upper'] = xtest['answer_num_words_upper'] / (xtest['a_words'].apply(len))\n",
    "\n",
    "\n",
    "xtrain.drop(['q_words', 'a_words', 'question_body_num_words_upper', \n",
    "             'answer_num_words_upper', 'question_body_num_punctuations'], axis = 1, inplace = True)\n",
    "xtest.drop(['q_words', 'a_words', 'question_body_num_words_upper', \n",
    "            'answer_num_words_upper', 'question_body_num_punctuations'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FE - distance-based "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"../input/universalsentenceencoderlarge4/\"\n",
    "embed = hub.load(module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_title\n",
      "question_body\n",
      "answer\n"
     ]
    }
   ],
   "source": [
    "embeddings_train = {}\n",
    "embeddings_test = {}\n",
    "for text in ['question_title', 'question_body', 'answer']:\n",
    "    print(text)\n",
    "    train_text = xtrain[text].str.replace('?', '.').str.replace('!', '.').tolist()\n",
    "    test_text = xtest[text].str.replace('?', '.').str.replace('!', '.').tolist()\n",
    "    \n",
    "    curr_train_emb = []\n",
    "    curr_test_emb = []\n",
    "    batch_size = 4\n",
    "    ind = 0\n",
    "    while ind*batch_size < len(train_text):\n",
    "        curr_train_emb.append(embed(train_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n",
    "        ind += 1\n",
    "        \n",
    "    ind = 0\n",
    "    while ind*batch_size < len(test_text):\n",
    "        curr_test_emb.append(embed(test_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n",
    "        ind += 1    \n",
    "        \n",
    "    embeddings_train[text + '_embedding'] = np.vstack(curr_train_emb)\n",
    "    embeddings_test[text + '_embedding'] = np.vstack(curr_test_emb)\n",
    "    \n",
    "del embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_dist = lambda x, y: np.power(x - y, 2).sum(axis=1)\n",
    "\n",
    "cos_dist = lambda x, y: (x*y).sum(axis=1)\n",
    "\n",
    "dist_features_train = np.array([\n",
    "    l2_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n",
    "    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n",
    "    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n",
    "    cos_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n",
    "    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n",
    "    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding'])\n",
    "]).T\n",
    "\n",
    "dist_features_test = np.array([\n",
    "    l2_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n",
    "    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n",
    "    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n",
    "    cos_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n",
    "    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n",
    "    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding'])\n",
    "]).T\n",
    "\n",
    "del embeddings_train, embeddings_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(0,6):\n",
    "    xtrain['dist'+str(ii)] = dist_features_train[:,ii]\n",
    "    xtest['dist'+str(ii)] = dist_features_test[:,ii]\n",
    "\n",
    "del dist_features_train, dist_features_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline buildup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_char = 10000\n",
    "limit_word = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_col = 'question_title'\n",
    "title_transformer = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(lowercase = False, max_df = 0.3, min_df = 2,\n",
    "                             binary = False, use_idf = True, smooth_idf = False,\n",
    "                             ngram_range = (1,2), stop_words = 'english', \n",
    "                             token_pattern = '(?u)\\\\b\\\\w+\\\\b' , max_features = 1000 ))\n",
    "])\n",
    "\n",
    "        \n",
    "title_transformer2 = Pipeline([\n",
    " ('tfidf2',  TfidfVectorizer( sublinear_tf=True,\n",
    "    strip_accents='unicode', analyzer='char',\n",
    "    stop_words='english', ngram_range=(1, 4), max_features= 1000))   \n",
    "])\n",
    "\n",
    "\n",
    "body_col = 'question_body'\n",
    "body_transformer = Pipeline([\n",
    "    ('tfidf',TfidfVectorizer(lowercase = False, max_df = 0.3, min_df = 2,\n",
    "                             binary = False, use_idf = True, smooth_idf = False,\n",
    "                             ngram_range = (1,2), stop_words = 'english', \n",
    "                             token_pattern = '(?u)\\\\b\\\\w+\\\\b' , max_features = limit_word ))\n",
    "])\n",
    "\n",
    "\n",
    "body_transformer2 = Pipeline([\n",
    " ('tfidf2',  TfidfVectorizer( sublinear_tf=True,\n",
    "    strip_accents='unicode', analyzer='char',\n",
    "    stop_words='english', ngram_range=(1, 4), max_features= limit_char))   \n",
    "])\n",
    "\n",
    "answer_col = 'answer'\n",
    "\n",
    "answer_transformer = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(lowercase = False, max_df = 0.3, min_df = 2,\n",
    "                             binary = False, use_idf = True, smooth_idf = False,\n",
    "                             ngram_range = (1,2), stop_words = 'english', \n",
    "                             token_pattern = '(?u)\\\\b\\\\w+\\\\b' , max_features = limit_word ))\n",
    "])\n",
    "\n",
    "answer_transformer2 = Pipeline([\n",
    " ('tfidf2',  TfidfVectorizer( sublinear_tf=True,\n",
    "    strip_accents='unicode', analyzer='char',\n",
    "    stop_words='english', ngram_range=(1, 4), max_features= limit_char))   \n",
    "])\n",
    "\n",
    "num_cols = [\n",
    "    'question_title_word_len', 'question_body_word_len', 'answer_word_len', 'answer_div',\n",
    "    'question_title_num_chars','question_body_num_chars','answer_num_chars',\n",
    "    'question_title_num_stopwords','question_body_num_stopwords','answer_num_stopwords',\n",
    "    'question_title_num_punctuations','question_body_num_punctuations','answer_num_punctuations',\n",
    "    'question_title_num_words_upper','question_body_num_words_upper','answer_num_words_upper',\n",
    "    'dist0', 'dist1', 'dist2', 'dist3', 'dist4',       'dist5']\n",
    "# ,] + [f for f in xtrain.columns if 'db_' in f]\n",
    "\n",
    "num_transformer = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "    ('scale', PowerTransformer(method='yeo-johnson'))\n",
    "])\n",
    "\n",
    "\n",
    "cat_cols = [\n",
    "    'dom_0', \n",
    "    'dom_1', \n",
    "    'dom_2', \n",
    "    'dom_3',     \n",
    "    'category', \n",
    "    'is_question_no_name_user',\n",
    "    'is_answer_no_name_user',\n",
    "    'dom_cnt'\n",
    "]\n",
    "\n",
    "cat_transformer = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value='')),\n",
    "    ('encode', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('title', title_transformer, title_col),\n",
    "        ('title2', title_transformer2, title_col),\n",
    "        ('body', body_transformer, body_col),\n",
    "        ('body2', body_transformer2, body_col),\n",
    "        ('answer', answer_transformer, answer_col),\n",
    "        ('answer2', answer_transformer2, answer_col),\n",
    "        ('num', num_transformer, num_cols),\n",
    "        ('cat', cat_transformer, cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('estimator',Ridge(random_state=RANDOM_STATE, normalize = True))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created in previous version - just uploaded here \n",
    "vector_as = pd.read_csv('../input/alphas-vector/alphas_vector.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep\n",
    "id_train = xtrain['qa_id']\n",
    "ytrain = xtrain[target_cols]\n",
    "xtrain.drop(target_cols + ['qa_id'], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "id_test = xtest['qa_id'] \n",
    "xtest.drop('qa_id', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropcols = ['question_user_name', 'question_user_page',\n",
    " 'answer_user_name', 'answer_user_page','url','host']\n",
    "\n",
    "xtrain.drop(dropcols, axis = 1, inplace = True)\n",
    "xtest.drop(dropcols, axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_title</th>\n",
       "      <th>question_body</th>\n",
       "      <th>answer</th>\n",
       "      <th>category</th>\n",
       "      <th>question_title_word_len</th>\n",
       "      <th>question_body_word_len</th>\n",
       "      <th>answer_word_len</th>\n",
       "      <th>is_question_no_name_user</th>\n",
       "      <th>is_answer_no_name_user</th>\n",
       "      <th>answer_div</th>\n",
       "      <th>...</th>\n",
       "      <th>question_prop_punctuations</th>\n",
       "      <th>answer_prop_punctuations</th>\n",
       "      <th>question_prop_upper</th>\n",
       "      <th>answer_prop_upper</th>\n",
       "      <th>dist0</th>\n",
       "      <th>dist1</th>\n",
       "      <th>dist2</th>\n",
       "      <th>dist3</th>\n",
       "      <th>dist4</th>\n",
       "      <th>dist5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What am I losing when using extension tubes in...</td>\n",
       "      <td>After playing around with macro photography on...</td>\n",
       "      <td>I just got extension tubes, so here's the skin...</td>\n",
       "      <td>LIFE_ARTS</td>\n",
       "      <td>13</td>\n",
       "      <td>141</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040670</td>\n",
       "      <td>0.036014</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.834191</td>\n",
       "      <td>0.570403</td>\n",
       "      <td>0.780612</td>\n",
       "      <td>0.582905</td>\n",
       "      <td>0.714799</td>\n",
       "      <td>0.609694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the distinction between a city and a s...</td>\n",
       "      <td>I am trying to understand what kinds of places...</td>\n",
       "      <td>It might be helpful to look into the definitio...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>16</td>\n",
       "      <td>142</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038023</td>\n",
       "      <td>0.031042</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>1.487265</td>\n",
       "      <td>0.694513</td>\n",
       "      <td>1.208260</td>\n",
       "      <td>0.256368</td>\n",
       "      <td>0.652744</td>\n",
       "      <td>0.395870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Maximum protusion length for through-hole comp...</td>\n",
       "      <td>I'm working on a PCB that has through-hole com...</td>\n",
       "      <td>Do you even need grooves?  We make several pro...</td>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>7</td>\n",
       "      <td>118</td>\n",
       "      <td>183</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.644809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030628</td>\n",
       "      <td>0.021947</td>\n",
       "      <td>0.084507</td>\n",
       "      <td>0.036036</td>\n",
       "      <td>1.348029</td>\n",
       "      <td>0.838107</td>\n",
       "      <td>1.223488</td>\n",
       "      <td>0.325985</td>\n",
       "      <td>0.580947</td>\n",
       "      <td>0.388256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      question_title  \\\n",
       "0  What am I losing when using extension tubes in...   \n",
       "1  What is the distinction between a city and a s...   \n",
       "2  Maximum protusion length for through-hole comp...   \n",
       "\n",
       "                                       question_body  \\\n",
       "0  After playing around with macro photography on...   \n",
       "1  I am trying to understand what kinds of places...   \n",
       "2  I'm working on a PCB that has through-hole com...   \n",
       "\n",
       "                                              answer   category  \\\n",
       "0  I just got extension tubes, so here's the skin...  LIFE_ARTS   \n",
       "1  It might be helpful to look into the definitio...    CULTURE   \n",
       "2  Do you even need grooves?  We make several pro...    SCIENCE   \n",
       "\n",
       "   question_title_word_len  question_body_word_len  answer_word_len  \\\n",
       "0                       13                     141              150   \n",
       "1                       16                     142               74   \n",
       "2                        7                     118              183   \n",
       "\n",
       "   is_question_no_name_user  is_answer_no_name_user  answer_div  ...  \\\n",
       "0                         0                       0    0.680000  ...   \n",
       "1                         0                       0    0.824324  ...   \n",
       "2                         0                       0    0.644809  ...   \n",
       "\n",
       "   question_prop_punctuations answer_prop_punctuations question_prop_upper  \\\n",
       "0                    0.040670                 0.036014            0.081633   \n",
       "1                    0.038023                 0.031042            0.034483   \n",
       "2                    0.030628                 0.021947            0.084507   \n",
       "\n",
       "  answer_prop_upper     dist0     dist1     dist2     dist3     dist4  \\\n",
       "0          0.090909  0.834191  0.570403  0.780612  0.582905  0.714799   \n",
       "1          0.039216  1.487265  0.694513  1.208260  0.256368  0.652744   \n",
       "2          0.036036  1.348029  0.838107  1.223488  0.325985  0.580947   \n",
       "\n",
       "      dist5  \n",
       "0  0.609694  \n",
       "1  0.395870  \n",
       "2  0.388256  \n",
       "\n",
       "[3 rows x 35 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvalid1 = np.zeros((xtrain.shape[0], len(target_cols)))\n",
    "mfull1 = np.zeros((xtest.shape[0], len(target_cols)))\n",
    "\n",
    "\n",
    "mvalid2 = np.zeros((xtrain.shape[0], len(target_cols)))\n",
    "mfull2 = np.zeros((xtest.shape[0], len(target_cols)))\n",
    "\n",
    "kf = KFold(n_splits = nfolds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'question_title_num_chars' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-e0d3141ea144>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mbe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mbe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvector_as\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mbe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ridge_f'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_col'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.sav'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \"\"\"\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    354\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                 **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    318\u001b[0m             \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_transformers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_column_callables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_remainder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_fit_transform_one\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m_validate_remainder\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0mcols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_column_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0mremaining_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0mremaining_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mall_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         raise ValueError(\"No valid specification of the columns. Only a \"\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mall_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         raise ValueError(\"No valid specification of the columns. Only a \"\n",
      "\u001b[0;31mValueError\u001b[0m: 'question_title_num_chars' is not in list"
     ]
    }
   ],
   "source": [
    "for wfold, (train_index, test_index) in enumerate(kf.split(xtrain)):\n",
    "\n",
    "    \n",
    "    print('---')\n",
    "    # split\n",
    "    x0, x1 = xtrain.loc[train_index], xtrain.loc[test_index]\n",
    "    y0, y1 = ytrain.loc[train_index], ytrain.loc[test_index]\n",
    "\n",
    "    for ii in range(0, ytrain.shape[1]):\n",
    "\n",
    "        # fit model\n",
    "        be = clone(pipeline)\n",
    "        be.steps[1][1].alpha = vector_as.loc[ii]\n",
    "        be.fit(x0, np.array(y0)[:,ii])\n",
    "\n",
    "        filename = 'ridge_f'+str(wfold)+'_col'+str(ii)+'.sav'\n",
    "        pickle.dump(be, open(filename, 'wb'))\n",
    "        \n",
    "        # park forecast\n",
    "        mvalid[test_index, ii] = be.predict(x1)\n",
    "        mfull[:,ii] += be.predict(xtest)/kf.n_splits\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mvalid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-572ab47463c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcorvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmvalid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrankdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmvalid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmfull\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrankdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmfull\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmfull\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mvalid' is not defined"
     ]
    }
   ],
   "source": [
    "corvec = np.zeros((ytrain.shape[1],1))\n",
    "for ii in range(0, ytrain.shape[1]):\n",
    "    mvalid[:,ii] = rankdata(mvalid[:,ii])/mvalid.shape[0]\n",
    "    mfull[:,ii] = rankdata(mfull[:,ii])/mfull.shape[0]\n",
    "    \n",
    "    corvec[ii] = stats.spearmanr(ytrain[ytrain.columns[ii]], mvalid[:,ii])[0]\n",
    "    \n",
    "print(corvec.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mvalid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-17039e100fa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'qa_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'qa_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetas_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'prval_ridge_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtodate\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mvalid' is not defined"
     ]
    }
   ],
   "source": [
    "prval = pd.DataFrame(mvalid)\n",
    "prval.columns = ytrain.columns\n",
    "prval['qa_id'] = id_train\n",
    "prval = prval[['qa_id'] + list(prval.columns[:-1])]\n",
    "prval.to_csv(metas_dir + 'prval_ridge_'+todate+ '.csv', index = False)\n",
    "\n",
    "\n",
    "prfull = pd.DataFrame(mfull)\n",
    "prfull.columns = ytrain.columns\n",
    "prfull['qa_id'] = id_test\n",
    "prfull = prfull[['qa_id'] + list(prfull.columns[:-1])]\n",
    "prfull.to_csv(metas_dir + 'prfull_ridge_'+todate+ '.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prfull' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-0367f5cfe1b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprfull\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'submission.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'prfull' is not defined"
     ]
    }
   ],
   "source": [
    "prfull.to_csv(sub_dir + 'submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
